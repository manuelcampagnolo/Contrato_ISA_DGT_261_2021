{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1c8bIBZoDiSA4NULqM1sNc3SBUAm2yVfT","timestamp":1668423831286}],"collapsed_sections":["5-n-63nOdU0r","_DaO_vDfV6n-","c5WFukfnKQs9","mdFtb4EWKZ1r","ZPvaJrEGdXdT","xR53NFCzss4F","WmimsOfOLd5l","ChHd4lHPMr9m","NTvXuUWUXO-j","r4CnvYXmpJWk","V_dhfVVmtl0x","ljuwDHo9xSrg","Dso6RpMXtCrI","QiAdeW7Iticc","CMfzf9YHzuWz","qo9tuWmpKIGN","aLoHNlP49tGC","j9sYzxEf0eFF","U79FQZ1OOpJn","9ooZhREHlV77","vZjKx6H8cblw","WZRNApYm77vP","RGUQ8jslUp-N","KsBCOZDcuB2Q","gxJccnvct-yx","3Z37BYXqt26h","nTSOl_dRtxtx","fMDGSjIUqxM-","XsyVNv4zq4QI","RFXJfrsIq99I"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Imports"],"metadata":{"id":"5-n-63nOdU0r"}},{"cell_type":"code","source":["import subprocess\n","import sys\n","\n","def install(package):\n","    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n","install('geopandas')\n","install('geemap')\n","install('haversine') # Não sei se é assim que deve ser feito\n","install('rtree')\n","install('pygeos')\n","install('rasterio')\n","\n","import geopandas as gpd\n","import rasterio\n","import rtree, pygeos, shapely\n","import ee, os, geemap, glob\n","import pandas as pd # lembrar de trocar para o pandas 1.3 (o mais novo)\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib\n","import seaborn as sns\n","import gc\n","import haversine as hs # Novo\n","from haversine.haversine import Unit # Novo\n","import datetime\n","import time\n","import random\n","from scipy.spatial import cKDTree\n","from shapely.geometry import Point\n","\n","ee.Authenticate() # quando este passo nao foi feito obtive erro no geemap.shp_to_ee\n","ee.Initialize()"],"metadata":{"id":"sK8CoziYdWf-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Funcoes base"],"metadata":{"id":"_DaO_vDfV6n-"}},{"cell_type":"markdown","source":["####Adicionar índices e filtrar imagens Sentinel 2"],"metadata":{"id":"c5WFukfnKQs9"}},{"cell_type":"code","source":["#compute ndvi\n","def addNDVI(image):\n","  ndvi = image.normalizedDifference(['B8', 'B4']).multiply(10000).int16()\n","  return image.addBands(ndvi.rename('ndvi'))\n","\n","#compute ndwi\n","def addNDWI(image):\n","  ndwi = image.normalizedDifference(['B3', 'B8']).multiply(10000).int16()\n","  return image.addBands(ndwi.rename('ndwi'))\n","\n","#compute EVI\n","def addEVI(image):\n","  evi = image.expression(\n","  '2.5 * ((NIR-RED) / (NIR + 6 * RED - 7.5* BLUE +1))', {\n","    'NIR':image.select('B8').divide(10000),\n","    'RED':image.select('B4').divide(10000),\n","    'BLUE':image.select('B2').divide(10000)    \n","  }).multiply(10000).int16()                                                        \n","  return image.addBands(evi.rename('evi'))\n","\n","#compute NBR\n","def addNBR(image):\n","  nbr = image.normalizedDifference(['B8', 'B12']).multiply(10000).int16()\n","  return image.addBands(nbr.rename('nbr'))\n","\n","#compute NBR2\n","def addNBR2(image):\n","  nbr = image.normalizedDifference(['B11', 'B12']).multiply(10000).int16()\n","  return image.addBands(nbr.rename('nbr2'))\n","\n","#SCL cloud/shadow filter\n","def filterS2_level2A(image):\n","  SCL = image.select('SCL')\n","  mask01 = ee.Image(0).where((SCL.lt(8)).And(SCL.gt(3)),1)   #Put a 1 on good pixels\n"," #(SCL.gt(3),1)\n","  return image.updateMask(mask01)\n","\n","#s2cloudless filter\n","def filterS2cloudless(S2SRCol, S2CloudCol):\n","\n","  CLOUD_FILTER = 60;\n","  CLD_PRB_THRESH = 50;\n","  NIR_DRK_THRESH = 0.2;\n","  CLD_PRJ_DIST = 1;\n","  BUFFER = 50;\n","\n","  #filter images based on cloudy percentage (metadata)\n","  S2SRCol = S2SRCol.filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', CLOUD_FILTER))\n","\n","  #join S2SR with S2CloudCol\n","  joined = ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(\n","      primary = S2SRCol,\n","      secondary = S2CloudCol,\n","      condition = ee.Filter.equals(\n","          leftField = 'system:index',\n","          rightField = 'system:index'\n","      )))\n","  \n","  def add_cloud_bands(img):\n","    #Get s2cloudless image, subset the probability band.\n","    cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n","    #Condition s2cloudless by the probability threshold value\n","    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')\n","    #Add the cloud probability layer and cloud mask as image bands\n","    return img.addBands(ee.Image([cld_prb, is_cloud]))\n","  \n","  def add_shadow_bands(img):\n","    #Identify water pixels from the SCL band\n","    not_water = img.select('SCL').neq(6)\n","    #Identify dark NIR pixels that are not water (potential cloud shadow pixels)\n","    SR_BAND_SCALE = 1e4\n","    dark_pixels = img.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')\n","    #Determine the direction to project cloud shadow from clouds (assumes UTM projection)\n","    shadow_azimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')))\n","    #Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input\n","    cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)\n","        .reproject(crs=img.select(0).projection(), scale=100)\n","        .select('distance')\n","        .mask()\n","        .rename('cloud_transform'))\n","    #Identify the intersection of dark pixels with cloud shadow projection\n","    shadows = cld_proj.multiply(dark_pixels).rename('shadows')\n","    #Add dark pixels, cloud projection, and identified shadows as image bands\n","    return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))\n","\n","  def add_cld_shdw_mask(img):\n","    #Add cloud component bands.\n","    img_cloud = add_cloud_bands(img)\n","    #Add cloud shadow component bands.\n","    img_cloud_shadow = add_shadow_bands(img_cloud)\n","    #Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n","    is_cld_shdw = img_cloud_shadow.select('clouds').add(img_cloud_shadow.select('shadows')).gt(0)\n","    #Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input\n","    #20 m scale is for speed, and assumes clouds don't require 10 m precision\n","    is_cld_shdw = (is_cld_shdw.focalMin(2).focalMax(BUFFER*2/20)\n","        .reproject(crs=img.select([0]).projection(), scale= 20)\n","        .rename('cloudmask'))\n","    #Add the final cloud-shadow mask to the image\n","    return img_cloud_shadow.addBands(is_cld_shdw)\n","\n","  def apply_cld_shdw_mask(img):\n","    #Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.\n","    not_cld_shdw = img.select('cloudmask').Not();\n","    #Subset reflectance bands and update their masks, return the result.\n","    return img.updateMask(not_cld_shdw)\n","\n","  s2_sr = joined.map(add_cld_shdw_mask).map(apply_cld_shdw_mask)\n","\n","  return s2_sr\n","\n"],"metadata":{"id":"ovXFiDfqKRk9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####getImageCollection - Carregar coleção de imagens Sentinel-2"],"metadata":{"id":"mdFtb4EWKZ1r"}},{"cell_type":"code","source":["def getImageCollection(params_ImgCol, sites, buf=False, clip=True):\n","  \n","  \"\"\"Function that takes the collection name, start and end dates and\n","  a geometry that is going to be used to clip the image.\n","  Returns an image collection for the bounding box that contains the points defined in sites \n","  (and buffer if that is the case)\n","  Arguments: \n","  params_ImgCol - dictionary containing:\n","    date_start/date_end - dates used as temporal filter;\n","    indices - indices that must be added to the images (only supports NDVI, NDWI and EVI);\n","  sites - geometry used to get the spatial location and also clip the images;\n","  clip - whether to clip the image collection to the extent of the sites geometry;\n","  buf - buffer size used in case of analysis of area surrounding a point.\"\"\"\n","\n","  #define spatial extent\n","  if buf: # valor do buffer para o caso de procurar a informação do entorno\n","    export_extent = sites.geometry().buffer(buf).bounds()\n","  else:\n","    #check if sites is a feature collection. If not (meaning it's a single point), the method bounds should not be used\n","    if ee.Algorithms.ObjectType(sites).getInfo() == 'FeatureCollection':\n","      export_extent = sites.geometry().bounds()\n","    else:\n","      export_extent = sites.geometry()\n","\n","  #get collection by its name and filter with the spatial extent of the sites and dates\n","  S2 = ee.ImageCollection(params_ImgCol['nameImage']).filterBounds(export_extent).filterDate(params_ImgCol['date_start'],params_ImgCol['date_end'])\n","  \n","  #filter clouds\n","  if params_ImgCol['cloudFilter'] == 'SCL': #apply cloud filter based on SCL values\n","    S2filtered= S2.map(filterS2_level2A)\n","\n","  elif params_ImgCol['cloudFilter'] == 's2cloudless': #apply cloud filter based on the s2cloudless procedure\n","    #load cloud probability collection\n","    s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY').filterBounds(export_extent)\n","                                                                            .filterDate(params_ImgCol['date_start'],params_ImgCol['date_end']))\n","    S2filtered = filterS2cloudless(S2, s2_cloudless_col)\n","\n","  #check and add spectral indices entered by user (NDVI, NDWI, EVI, NBR) \n","  if 'ndvi' in list(map(lambda x: x.lower(), params_ImgCol['indices'])):\n","    S2filtered = S2filtered.map(addNDVI)\n","  if 'ndwi' in list(map(lambda x: x.lower(), params_ImgCol['indices'])):\n","    S2filtered = S2filtered.map(addNDWI)\n","  if 'evi' in list(map(lambda x: x.lower(), params_ImgCol['indices'])):\n","    S2filtered = S2filtered.map(addEVI)\n","  if 'nbr' in list(map(lambda x: x.lower(), params_ImgCol['indices'])):\n","    S2filtered = S2filtered.map(addNBR)\n","  if 'nbr2' in list(map(lambda x: x.lower(), params_ImgCol['indices'])):\n","    S2filtered = S2filtered.map(addNBR2)\n","\n","  for i in params_ImgCol['indices']:\n","    if i.lower() not in ['ndvi','ndwi','evi','nbr','nbr2']:\n","      print('Adding indice {} is currently not supported on our code. The returned collection will lack such indice'.format(i))\n","\n","  #function to clip image to selected region.\n","  def Clip(image):\n","    clipImage = image.clip(export_extent)\n","    return clipImage\n","\n","  #clip to selected region\n","  if clip:\n","    S2filtered_clipped = S2filtered.map(Clip) #colecao de imagens utilizadas\n","  \n","  return S2filtered_clipped if clip else S2filtered \n","  "],"metadata":{"id":"_aUb0Tk3KZKV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####runCCDC"],"metadata":{"id":"_Va3S-pWjW-B"}},{"cell_type":"code","source":["def runCCDC(collection, bandas, params_ccdc):\n","  \"\"\"\n","  Entrada:\n","   - collection: objeto Image Collection\n","   - bandas: lista com as bandas selecionadas para a analise\n","   - params_ccdc: dicionario com os parametros do ccdc\n","  Saida:\n","   - ccdc_result: Image Array do resultado do CCDC  \n","  \"\"\"\n","  \n","  # B2-Blue, B3-Green, B4-Red, B8-NIR, B12-SWIR2\n","  ccdc_result = ee.Algorithms.TemporalSegmentation.Ccdc(collection.select(bandas), #colecao\n","                                                params_ccdc['bandas_breakpoint'], #breakpointBands                                               \n","                                                params_ccdc['bandas_tmask'], #tmaskBands \n","                                                params_ccdc['minObs'], #minObservations\n","                                                params_ccdc['chiSquare'], #chiSquareProbability\n","                                                params_ccdc['minYears'], #minNumOfYearsScaler\n","                                                params_ccdc['dateForm'], #dateFormat \n","                                                params_ccdc['Lambda'], #lambda para NDVI normalizado * 10000\n","                                                params_ccdc['maxIter']) #maxIterations\n","  return ccdc_result"],"metadata":{"id":"8RSrQ3EIjZTQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Função filtro data frame CCDC; não usa dados dos analistas (apenas as datas)"],"metadata":{"id":"ZPvaJrEGdXdT"}},{"cell_type":"code","source":["def filterDate(pathDF, dataI, dataF,bandFilter, mag = None):\n","    \"\"\"\n","    Reduz o número de linhas do data frame de entrada, removendo as linhas fora do período de análise e\n","    para o limite estabelecido de magnitude máxima.\n","    Entrada:\n","        pathDF: caminho do Data Frame do CCDC\n","        dataI: String com a data inicial na forma = 'AAAA-MM-DD' (e.g. a data inicial dos analistas nos pontos DGT 300)\n","        dataF: String com a data final na forma = 'AAAA-MM-DD' (e.g. a data final dos analistas nos pontos DGT 300)\n","        bandFilter: String com a banda para a qual se deseja filtrar os dados. A esta banda é aplicado o criterio do mag.\n","        mag: Número com o limite da magnitude, e.g 0 só serão utilizadas as linhas com magnitudo menor ou igual a zero\n","    Saída:\n","        Data Frame filtrado\n","    \"\"\"\n","    # Data Frame CCDC\n","    if pathDF.endswith('.csv'):\n","        df = pd.read_csv(pathDF, delimiter = ';')\n","    if pathDF.endswith('.pkl'):\n","        df = pd.read_pickle(pathDF)\n","        \n","    for dtCol in df.columns:\n","        if 'tBreak' in dtCol or 'tEnd' in dtCol or 'tStart' in dtCol:\n","            mask = df.loc[:, dtCol] == 0\n","            df[dtCol] = pd.to_datetime(df[dtCol], unit = 'ms')\n","            df.loc[mask, dtCol] = np.nan\n","        elif 'End_S' in dtCol:\n","            df[dtCol] = pd.to_datetime(df[dtCol]) # Esta coluna inicialmente esta em formato texto\n","    df.rename(columns={ 'Unnamed: 0':'IDCCDC'}, inplace=True)  \n","    \n","    if mag != None:\n","        # caso haja magnitude limite, colocar tudo como NAT que seja acima deste limite\n","        df.loc[df[bandFilter] > mag, 'tBreak'] = pd.to_datetime(np.nan)\n","        df = df.copy()\n","    else:\n","        df = df.copy()\n","        \n","    # filtro das datas\n","    yi, mi, diai = convertDate(dataI)\n","    fltInicial = datetime.datetime(yi, mi, diai)\n","    yf, mf, diaf = convertDate(dataF)\n","    fltFinal = datetime.datetime(yf, mf, diaf)           \n","    \n","    # 1 Adiciona a coluna com a menor data de start do fit\n","    df['startMin'] = df.groupby(['coord_ccdc'])['tStart'].transform('min')\n","\n","    # 2 Adiciona o número de breaks existentes num grupo de IDCCDC, independente de fltInicial e fltFinal\n","    df['numBreak'] = np.ceil(df.groupby(['coord_ccdc'])['changeProb'].transform('sum'))\n","    \n","    # Colocar Nat nas probabilidades fracionadas\n","    df.loc[((df.changeProb > 0) & (df.changeProb < 1)), 'tBreak'] = pd.to_datetime(np.nan)\n","\n","    # 3 Verifica se se os breaks estão dentro do período de análise e transforma em NaT todos os que não estão\n","    df['breaks_in_tmask'] = (~df.tBreak.isnull()).astype(int)\n","    df.loc[(df['tBreak'] <= fltInicial) | (df['tBreak'] >= fltFinal), 'breaks_in_tmask'] = 0\n","    df.loc[(df['tBreak'] <= fltInicial) | (df['tBreak'] >= fltFinal), 'tBreak'] = np.nan\n","\n","    # Mascaras necessárias\n","    # a) Verifica os breaks NaT para as linhas com mais de 1 break\n","    mask = pd.Series(np.zeros(len(df),dtype=bool),index = df.index)\n","    mask.loc[(df.tBreak.isnull()) & (df.numBreak > 1)]= True #cond3\n","    \n","    # b) Verifica nas linhas de 1 break e sejam nulos qual é aquele que tem o início da série,\n","    #pois caso esteja fora da data de análise deve ser eliminado\n","    nmask = pd.Series(np.zeros(len(df),dtype=bool),index = df.index)\n","    nmask.loc[(df.tBreak.isnull()) & (df.numBreak == 1) & (df.breaks_in_tmask == 0) & (df.tStart == df.startMin)]= True    \n","    \n","    # Aplica as mascaras acima e gera um novo DF\n","    subset_Filtro = df[((mask == False) & (nmask == False))].copy()\n","    \n","    # c) Calcula quantos linhas há por IDCCDC e caso ainda existam 2 significa que o break está dentro do período de análise e o fit final, sem break\n","    # deve ser eliminado\n","    smask = pd.Series(np.zeros(len(subset_Filtro),dtype=bool),index = subset_Filtro.index)\n","    smask.loc[(subset_Filtro.groupby(['coord_ccdc'])['IDCCDC'].transform('count') == 2) & (subset_Filtro.changeProb == 0) & (df.numBreak == 1)] = True\n","    subset_Filtro = subset_Filtro[(smask == False)].copy()\n","    \n","    # d) Para os IDCCDC que apresentam linhas com probabilidade fracionada, mantem esta linha, no caso de todas estarem fora do período de análise\n","    pmask = pd.Series(np.zeros(len(df),dtype=bool),index = df.index)\n","    pmask.loc[~((df.changeProb > 0) & (df.changeProb < 1) & (df.tBreak.isnull()) & (df.groupby(['coord_ccdc'])['tBreak'].transform('count') == 0))]=True\n","    subset_Filtro = subset_Filtro.append(df[pmask == False])\n","    \n","    # e) Para os IDCCDC que tem mais de um break e todos estao fora do periodo e devemos manter o fit final\n","    fmask = pd.Series(np.zeros(len(df),dtype=bool),index = df.index)\n","    fmask.loc[((df.changeProb == 0) & (df.numBreak > 1) & (df.tBreak.isnull()) & (df.groupby(['coord_ccdc'])['tBreak'].transform('count') == 0))]=True\n","    subset_Filtro = subset_Filtro.append(df[fmask])\n","    \n","\n","    return subset_Filtro"],"metadata":{"id":"1Z9Z_EJVd-Us"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Funcoes Gerar Graficos"],"metadata":{"id":"xR53NFCzss4F"}},{"cell_type":"markdown","source":["####ccdcTimeseries - Gerar e selecionar os segmentos de reta; by parevalo"],"metadata":{"id":"WmimsOfOLd5l"}},{"cell_type":"code","source":["def ccdcTimeseries(collection, proj, dateFormat, ccdc, geometry, band, padding):\n","\n","  \"\"\"Calculate and returns a time series of CCDC values for the dates with observations based on the harmonic coefficients\"\"\"\n","\n","  def harmonicFit(t, coef):\n","    #returns the result of the harmonic regression for a given time t, given the coefficients' values\n","    PI2 = 2.0 * np.pi\n","    OMEGAS = [PI2 / 365.25, PI2, PI2 / (1000 * 60 * 60 * 24 * 365.25)]\n","    omega = OMEGAS[dateFormat]\n","    return coef.get([0]).add(coef.get([1]).multiply(t)).add(coef.get([2]).multiply(t.multiply(omega).cos())).add(coef.get([3]).multiply(t.multiply(omega).sin())).add(coef.get([4]).multiply(t.multiply(omega * 2).cos())).add(coef.get([5]).multiply(t.multiply(omega * 2).sin())).add(coef.get([6]).multiply(t.multiply(omega * 3).cos())).add(coef.get([7]).multiply(t.multiply(omega * 3).sin()))\n","  \n","  def convertDateFormat(date, format):\n","    #converts date to the intended format\n","    if format == 0:\n","      epoch = 719529\n","      days = date.difference(ee.Date('1970-01-01'), 'day')\n","      return days.add(epoch)\n","    elif format == 1:\n","      year = date.get('year')\n","      fYear = date.difference(ee.Date.fromYMD(year, 1, 1), 'year')\n","      return year.add(fYear)\n","    else:\n","      return date.millis()\n","\n","  def date_to_segment(t, fit):\n","    tStart = ee.Array(fit.get('tStart'))\n","    tEnd = ee.Array(fit.get('tEnd'))\n","    return tStart.lte(t)and(tEnd.gte(t)).toList().indexOf(1)\n","\n","  def produceTimeSeries(collection, ccdc, geometry, band):\n","    ccdcFits = ccdc.reduceRegion(reducer=ee.Reducer.first(), geometry=geometry, crs= proj)\n","    if padding:\n","      collection = collection.sort('system:time_start')\n","      first = collection.first()\n","      last = collection.sort('system:time_start', False).first()\n","      def aux1(t):\n","        fYear = ee.Number(t)\n","        year = fYear.floor()\n","        return  ee.Date.fromYMD(year, 1, 1).advance(fYear.subtract(year), 'year')\n","\n","      fakeDates = ee.List.sequence(first.date().get('year'), last.date().get('year'), padding).map(aux1)\n","      def aux2(d):\n","        return ee.Image().rename(band).set('system:time_start', ee.Date(d).millis())\n","\n","      fakeDates = fakeDates.map(aux2)\n","      collection = collection.merge(fakeDates)\n","       \n","    collection = collection.sort('system:time_start')\n","  \n","    #Augment images with the model fit\n","    def aux3(img):\n","      time = convertDateFormat(img.date(), dateFormat)\n","      segment = date_to_segment(time, ccdcFits)\n","      value = img.select(band).reduceRegion(reducer=ee.Reducer.first(), geometry=geometry, crs=proj).getNumber(band)\n","      coef = ee.Algorithms.If(segment.add(1), ccdcFits.getArray(band + '_coefs').slice(0, segment, segment.add(1)).project([1]),ee.Array([0,0,0,0,0,0,0,0,0]))\n","      fit = harmonicFit(time, ee.Array(coef))\n","\n","      #return img.set(value=value, fitTime=time, fit=fit, coef=coef, segment=segment, dateString=img.date().format(\"YYYY-MM-dd\")).set(segment.format(\"h%d\"), fit)\n","      return img.set({'value':value, 'fitTime':time, 'fit':fit, 'coef':coef, 'segment':segment, 'dateString':img.date().format(\"YYYY-MM-dd\")}).set(segment.format(\"h%d\"), fit)\n","    \n","    timeSeries = collection.map(aux3)      \n","    return timeSeries  \n","  return produceTimeSeries(collection, ccdc, geometry, band)"],"metadata":{"id":"ik2ZarItLDVW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####chartCcdc - Dados para o chart"],"metadata":{"id":"ChHd4lHPMr9m"}},{"cell_type":"code","source":["def chartCcdc(collection, ccdc_result, runParams, geometry, band): # aqui entraria a banda\n","  \n","  \"\"\"\n","  Returns a dataframe containing a time series with values from actual observations and the respective CCDC values.\n","  Also provides information about each segment's coefficients.\n","  Dataframe structure: date - observation date; \n","  obs - observation value; \n","  a, b, c, d, e, f - CCDC value for fits 1 (a), 2 (b), 3 (c)...\n","  g - CCDC value regardless of fit.\n","  \n","  Arguments: collection - ee Image collection;\n","  ccdc_result - output from ccdc ee algorithm;\n","  runParams - dictionary containing run info. In this case it needs the key nSegs, that indicates the desired number of segments;\n","  geometry - the geometry (ee.geometry.Geometry) of the point where one wants to obtain the CCDC chart data;\n","  band - the band of interest;\n","  \"\"\"\n","\n","  # Set up and run CCDC\n","  # Need to filter bands because indices code does not currently work if TEMP is included\n","  #collection = S2filtered_clipped.select(bandas)\n","  #collection = collection.select(['ndvi', 'B2', 'B3','B4','B8', 'B11', 'B12'])\n","    \n","  # Snap click box to image\n","  ref_image =ee.Image(collection.select(band).first()) \n","  proj = ref_image.projection().atScale(10)\n","  \n","  series = ccdcTimeseries(collection, proj, 2, ccdc_result, geometry, band, 0.1) # alterar a banda\n","\n","  def aux4(p):\n","    return ee.Number(p).floor()\n","  c1 = geometry.transform(proj, 1).coordinates().map(aux4)\n","  def aux5(p):\n","    return ee.Number(p).add(1)\n","  c2 = c1.map(aux5)\n","  p2 =  ee.Geometry.LineString([c1, c2], proj)\n","  \n","  # Get required list programatically for n segments\n","  templist = [\"dateString\", \"value\" ]\n","  for i in range(0, runParams['nSegs']): \n","    templist.append(\"h\"+str(i))\n","\n","  templist.append(\"fit\")\n","  listLength = len(templist)\n","  \n","  table = series.reduceColumns(ee.Reducer.toList(listLength, listLength), templist).get('list')\n","  #print(table.getInfo())\n","  df = pd.DataFrame(table.getInfo())\n","  df.columns= ['date','obs','a','b','c','d','e','f','g']\n","  return df"],"metadata":{"id":"xqJfmevNLhul"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####runChartccdc - Run chart CCDC"],"metadata":{"id":"NTvXuUWUXO-j"}},{"cell_type":"code","source":["def runChartccdc(collection, ccdc_result, runParams, point, band, validation=None, ponto_qualquer=False):\n","\n","  \"\"\"\n","  Returns a dictionary whose key is the point and whose value is another dictionary containing the dataframe with time series (value)\n","  of a given band (key).\n","  Arguments: collection - ee Image collection;\n","  runParams - dictionary containing run info. In this case it needs the key nSegs, that indicates the desired number of segments;\n","  point -  OBID number (for ponto_qualquer=False) or an ee.Feature from a specific chosen point (ponto_qualquer=True);\n","  band -  band on which the analysis is focused;\n","  validation - ee feature collection with validation points from DGT.\n","  \"\"\"\n","\n","  #check if the execution is targeted to the validation points or for a specific chosen point\n","  if ponto_qualquer == False: ##DELETAR APOS TESTES???\n","    #check if validation is a feature collection\n","    if ee.Algorithms.ObjectType(validation).getInfo() == 'FeatureCollection':\n","      #select feature from validation points based on the OBID\n","      feature = validation.filter(ee.Filter.eq('OBID', point))\n","    else: #assumes validation is an ee.feature (single point entered manually)\n","      feature = validation\n","  else:\n","    feature = point\n","    #if it's a point outside the validation dataset, variable point gets assigned to -1 to ensure compatibility with the rest of the code that was written for the validation dataset\n","    point = (feature.get('OBID')).getInfo()\n","        \n","  return {point:{band: chartCcdc(collection, ccdc_result, runParams, feature.geometry(), band)}}"],"metadata":{"id":"3b1IgUFiMtFF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####takeInfo - Informacao dos pontos de validacao para nomear o grafico"],"metadata":{"id":"r4CnvYXmpJWk"}},{"cell_type":"code","source":["def takeInfo(gpdOb, num):\n","\n","  \"\"\"\n","  Returns information regarding the change dates of each analyst and\n","  land cover type as given by COS. It is used to set the chart's filename and other\n","  chart elements.\n","  Arguments: \n","  gpdOb - geopandas dataframe corresponding to the validation shapefile;\n","  num - OBID number from which one wants to obtain info.\n","  \"\"\"\n","\n","  # dicionario para guardar os valores e possiveis combinacoes da tabela\n","  dic = {'bin':{}, 'tip':{}, 'dat':{}}  \n","  analistas = ['1_x','2_x','1_y','2_y','1_z','2_z']\n","  #filtrar o GDF\n","  filter = gpdOb.query('OBID == {0}'.format(num))  \n","  lat,log = (round(float(filter.geometry.y), 6), round(float(filter.geometry.x), 6))\n","  cobertura = filter.COS2018_Lg.values[0]\n","  # funcoes\n","  def binario(val):\n","    if val:\n","      valor = '1'\n","    else:\n","      valor = '0'\n","    return valor\n","  def alteracao(tipo):\n","    if tipo:\n","      alt = tipo\n","    else:\n","      alt = '-'\n","    return alt  \n","  for analista in analistas:       \n","    dic['bin'][analista] = binario(filter['tipo'+analista].values[0])\n","    dic['tip'][analista] = alteracao(filter['tipo'+analista].values[0])    \n","    dic['dat'][analista] = filter['data'+ analista].values[0]\n","  datas = dic['dat']\n","  alteracoes = dic['tip']\n","  arquivo = 'X{0}{1}_Y{2}{3}_Z{4}{5}'.format(dic['bin']['1_x'],dic['bin']['2_x'],dic['bin']['1_y'],\n","                                             dic['bin']['2_y'], dic['bin']['1_z'],dic['bin']['2_z'])\n","  return((cobertura,arquivo, datas,alteracoes, lat, log))"],"metadata":{"id":"ohXoweyvpK3a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####generateChart - Gerar os graficos"],"metadata":{"id":"V_dhfVVmtl0x"}},{"cell_type":"code","source":["def generateChart(dfs, band, point, gpd_validation, date_end, ponto_qualquer=False, export=False, output_path=False):\n","\n","    \"\"\"\n","    Function used to generate chart of observations and ccdc regression fits.\n","    Arguments: \n","    dfs - dictionary obtained from the runChartccdc function;\n","    band - the band in which the analysis in based on;\n","    point - either an OBID number or an ee.feature;\n","    gpd_validation - validation points shapefile read as a geodataframe;\n","    date_end - date of the end of the series;\n","    export - if set to True, function will export the chart figure to google drive; \n","    if set to False, it will only exhibit the chart. Note that when running the code\n","    for all validation points, export should be True.\n","    output_path - directory where charts will be exported to.    \n","    \"\"\"\n","\n","    #set point value in order to access the correct key in the dfs dictionary\n","    if ponto_qualquer == True:\n","      long_ponto = ee.Number(point.geometry().coordinates().get(0)).getInfo()\n","      lat_ponto = ee.Number(point.geometry().coordinates().get(1)).getInfo()\n","      point = (point.get('OBID')).getInfo()\n","\n","    #obid = list(df_elem.keys())[0]\n","    ts = dfs[point][band]\n","    #ts = dfs[obid]\n","\n","    #convert date column to the correct format\n","    ts['date'] = pd.to_datetime(ts['date'])\n","\n","    #get first date with observation\n","    first_date = ts[(ts['a'] > -10000) & (ts['obs'] > -10000)].iloc[0].date\n","    #delete rows with date prior to first date\n","    ts = ts[ts['date'] >= first_date]\n"," \n","    #remove empyt columns\n","    c = ts.columns\n","    for j in c:  \n","        if ts[j].count() == 0:\n","          ts = ts.drop(columns = [j])\n","\n","    #generate chart\n","    #configure chart elements\n","    sns.set_style(\"darkgrid\") #adds a grid to the chart\n","    leter = ['b', 'c', 'd','e', 'f'] #letras das colunas\n","    colors = ['green', 'purple', 'pink', 'yellow', 'orange'] # cores das linhas\n","    colunas = {*ts} - {'system:index', 'date', '.geo', 'g', 'a', 'obs'}\n","    plt.figure(figsize=(13, 6), dpi = 300)\n","    plt.scatter(ts['date'], ts['obs'], s=2.5, c = 'r', label= band.upper()) # era: 'NDVI'\n","    plt.plot(ts['date'],ts['a'], label='Fit1')\n","    for n, item in enumerate(colunas):\n","        if item in leter:\n","          Label = 'Fit{0}'.format(n+2)\n","          plt.plot(ts['date'],ts[str(leter[n])],c = colors[n], label='Fit{0}'.format(n+2))\n","\n","\n","    #This part runs only if the analysis is targeted to the validation points from DGT\n","    #if point != -1 :\n","    try:\n","      # get shapefile values and infos\n","      cosValue,analistasDGT, datasDGT,alteracoesDGT, latDGT, logDGT = takeInfo(gpd_validation, point)    \n","\n","      # insert DGT date in plot\n","      ptColors = ['red','green', 'purple', 'pink', 'black', 'orange']\n","      ptTypes =['s', 's', 'd', 'd', 'p','p' ]\n","      for ptN, k in enumerate(datasDGT.keys()):  ###################################\n","        if datasDGT[k] != None:\n","          if int(datasDGT[k]) > 0:          \n","            d = pd.to_datetime(datasDGT[k],format='%Y%m%d', errors='ignore')\n","            plt.scatter(d, ts['obs'].min(), s=20, c = ptColors[ptN],marker= ptTypes[ptN], label= '{0}: {1}'.format(k, alteracoesDGT[k]))\n","\n","      #numero de segmentos\n","      if colunas:\n","        segm = len(colunas) + 1\n","      else:\n","        segm = 1\n","      \n","      #set chart title\n","      plt.title('Cobertura COS: {0}. Lat: {1}, Long: {2}'.format(cosValue, latDGT, logDGT))\n","      #set chart filename\n","      chartName = 'OBID_{0}_{1}_{2}_{3}_Fits{4}_tMASK.png'.format(point, date_end.replace('-', ''), band.upper(), analistasDGT, segm)# OBS: date_end definido fora da funcao\n","    \n","    #configure axis labels and add legend\n","    #special rule for chart title and filename if the analysis is targeted to a specific user defined point\n","    #if point == -1:\n","      #plt.title('Análise de Ponto Específico (Lat: {0}, Long: {1})'.format(lat_ponto,long_ponto))\n","      #chartName = 'PontoEspecifico_{0}_{1}.png'.format(lat_ponto,long_ponto)\n","    except:\n","      plt.title('OBID: {}'.format(point))\n","      chartName = 'OBID_{0}_{1}_{2}_tMASK.png'.format(point, date_end.replace('-', ''), band.upper())\n","    #set other chart parameters that are applied regardless of the analysis target\n","    plt.xlabel('Data')\n","    plt.ylabel(band.upper()) # era 'NDVI'\n","    plt.legend()\n","\n","    \n","    #display chart on notebook cell or save (export) chart to drive    \n","    if export == True:\n","      matplotlib.use('agg') #switch matplotlib backend from inline to agg (prevents memory leak)\n","      plt.savefig(output_path + chartName) # caso nao tenha subpasta alterar\n","      print('Chart exported: ' + output_path + chartName)\n","      plt.close() #prevent displaying the charts in the notebook cell output\n","      plt.clf() #clear figure\n","      gc.collect() #attempt to free memory\n","    else:\n","      print(chartName)\n","      plt.show()"],"metadata":{"id":"LExLt4MitmLr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####chartAllpoints - Gerar para todos os pontos"],"metadata":{"id":"ljuwDHo9xSrg"}},{"cell_type":"code","source":["from threading import Thread\n","\n","#this is a Thread subclass that is modified so that it can return the function value when\n","#using threads\n","class ThreadWithReturnValue(Thread):\n","    def __init__(self, group=None, target=None, name=None,\n","                 args=(), kwargs={}, Verbose=None):\n","        Thread.__init__(self, group, target, name, args, kwargs)\n","        self._return = None\n","    def run(self):\n","        print(type(self._target))\n","        if self._target is not None:\n","            self._return = self._target(*self._args,\n","                                                **self._kwargs)\n","    def join(self, *args):\n","        Thread.join(self, *args)\n","        return self._return\n","\n","\n","def chartAllpoints(collection, ccdc_result, runParams, banda, ee_validation, gpd_validation, date_end, output_path):\n","\n","  \"\"\"Function used to generate charts for all points in the validation dataset.\n","  It runs the function runChartccdc for each OBID in blocks of 10. Then it stores the\n","  resulting dataframes in a dictionary (dfs). After that, it loops through all points\n","  in the dictionary and executes the generateChart function.\n","  Arguments: collection - ee Image collection;\n","  banda - band of analysis;\n","  date_end - date of the end of the series;\n","  ee_validation - validation dataset as ee feature collection;\n","  gpd_validation - validation dataset as a geodataframe;\n","  output_path - directory where the charts will be saved.\n","  \"\"\"\n","  backup_stdout = sys.stdout #create backup of stdout\n","  sys.stdout = open(os.devnull,'w') #this aims to avoid printing out some unnecessary text during the function execution\n","  dfs = {}\n","\n","  points_number = len(gpd_validation)\n","  #code used to run function runChartccdc in blocks of 10\n","  threads = []\n","  for i in range(0,points_number,5):\n","    threads = []\n","    for j in range(0,5):\n","      if i+j < points_number:\n","        t = ThreadWithReturnValue(target=runChartccdc, args=[collection, ccdc_result, runParams, i+j, banda, ee_validation])\n","        t.start()\n","        threads.append(t)\n","    for t in threads:\n","      thread_return = t.join()  \n","      dict_key = list(thread_return.keys())[0]\n","      dfs[dict_key] = thread_return[dict_key]\n","  \n","  #check if the execution crashed for some OBIDs (thread exception) and run again for the missing OBIDs\n","  missing = [x for x in dfs.keys() if x not in range(0,points_number)]\n","  while len(missing) > 0:\n","    for i in range(0,len(missing),5):\n","      threads = []\n","      for j in range(0,5):\n","        t = ThreadWithReturnValue(target=runChartccdc, args=[collection, ccdc_result, runParams, missing[i+j], banda, ee_validation])\n","        t.start()\n","        threads.append(t)\n","      for t in threads:\n","        thread_return = t.join()\n","        dict_key = list(thread_return.keys())[0]\n","        dfs[dict_key] = thread_return[dict_key]  \n","        \n","    missing = [x for x in dfs.keys() if x not in range(0,points_number)]\n","  \n","  sys.stdout = backup_stdout #restore stdout to normal, after being modified in the beginning of the function\n","\n","  #after filling up the dictionary dfs, run function generateChart for each point\n","  for i in range(points_number):\n","    if points_number == 1:\n","      generateChart(dfs, banda, i, gpd_validation, date_end, export=False, output_path=output_path)\n","    #by default, generation of charts for all points is intended to export chart to drive, not to exhibit it in notebook\n","    else:\n","      generateChart(dfs, banda, i, gpd_validation, date_end, export=True, output_path=output_path)\n","      gc.collect() #attempt to free memory\n","  "],"metadata":{"id":"Av7Y77-4xTr6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Funcoes Importar Info para Pontos"],"metadata":{"id":"R3xiS0tDs32S"}},{"cell_type":"markdown","source":["####getLatLong - ANTIGA"],"metadata":{"id":"Dso6RpMXtCrI"}},{"cell_type":"code","source":["# def getLatLong(area):\n","\n","#   \"\"\"Cria uma imagem contendo a lat long dos pixeis e recorta para a area determinada\"\"\"\n","\n","#   lat_long = ee.Image.pixelLonLat().clip(area) #versão antiga dessa linha utilizava uma função especifica criada por nós para fazer o clip\n","  \n","#   latDic = lat_long.reduceRegion(\n","#     reducer= ee.Reducer.toList(),\n","#     geometry= area,\n","#     scale= 10)\n","#   return pd.DataFrame(latDic.getInfo()) # retorna a informacao das coordenadas em um dataframe"],"metadata":{"id":"3NcBuHx3s_VM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Coletar info para os pontos - ccdcPoint, ccdcStudyArea & ccdcReducer"],"metadata":{"id":"QiAdeW7Iticc"}},{"cell_type":"code","source":["# AS DUAS PROXIMAS FUNCOES SAO PARA COLETAR O VALOR DO CCDC EM UM PONTO OU EM\n","#ÁREA. OBS: TENTEI FAZER COM O BUFFER 0 PARA VER SE O RESULTADO ERA IGUAL AO \n","# OBTIDO PARA SÓ UM PONTO, MAS NÃO DEU CERTO.\n","\n","def ccdcPoint(pointGeom, ccdcResult):\n","  \"\"\" \n","  Coletar as informacoes do CCDC para um ponto especifico\n","      input:\n","  pointGeom: Area do Ponto a reduzir o CCDC\n","  ccdcResult: Imagem resultante do algoritmo CCDC\n","      output:\n","  data frame contendo as informacoes no ponto selecionado\n","  \"\"\"\n","  point_study = ccdcResult.reduceRegion(\n","    reducer = ee.Reducer.first(), #usar quando quer 3 linhas      \n","    geometry = pointGeom.geometry(),      \n","    scale= 10)\n","  return pd.DataFrame(point_study.getInfo())\n","\n","def multipleCcdcPoint(fc, id_field, ccdc_result):\n","\n","  dfs = {}\n","  #code used to run function ccdcPoint in blocks of 10\n","  for i in range(0,fc.size().getInfo(),10):\n","    threads = []\n","    threads_return = []\n","    for j in range(0,10):\n","      t = ThreadWithReturnValue(target=ccdcPoint, args=[fc.filter(ee.Filter.eq(id_field, i+j)), ccdc_result])\n","      t.start()\n","      threads.append(t)\n","    for t in threads:\n","      threads_return.append(t.join())\n","    for k in range(len(threads_return)):\n","      dfs[i + k] = threads_return[k]\n","    \n","  \n","  return dfs\n","      \n","  \n","def ccdcStudyArea(area, ccdcResult):\n","  \"\"\"\n","  Coletar a informacao do CCDC para o entorno de um ponto especifico\n","    input:\n","    pointGeom: Geometria do ponto\n","    Buffer: Distancia do entorno do ponto para qual deseja-se a informacao\n","    ccdcResult: Resultado do ccdc\n","    output:\n","    dataframe contendo as informacoes sobre a area de estudada\n","  \"\"\"\n","  ccdc_study_area = ccdcResult.reduceRegion(    \n","    reducer= ee.Reducer.toList() , #usar quando quer 1 linha    \n","    geometry = area.bounds(),    \n","    scale= 10)\n","  return pd.DataFrame(ccdc_study_area.getInfo()) # Cria o Data Frame com a informacao de todos os pixeis"],"metadata":{"id":"j6gT58_Vt7qn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Na função ccdcReducer a seguir é feita a explosão do output do CCDC para obter a dataframe em que cada coluna corresponde a um elemento do output do CCDC."],"metadata":{"id":"5j1kRAu9kZjh"}},{"cell_type":"code","source":["def ccdcReducer(pointGeom, ccdcResult, dataFrame, endSerie, buf = False):\n","  \"\"\"\n","  Entrada:\n","  pointGeom - Geometria do Ponto\n","  ccdcResult - O resultado do CCDC\n","  dataFrame - O Data Frame que será sempre atualizado\n","  endSerie - data de final da serie (truncagem)\n","  buf - Caso desejar os valores ao redor do ponto\n","  Saida:\n","  Data Frame atualizado\n","  \"\"\"  \n","  if buf:\n","    areaEstudo = pointGeom.geometry().buffer(buf)\n","  else:\n","    areaEstudo = pointGeom.geometry()\n","  # desta forma sempre será gerada a coluna com listas, que será explodida abaixo\n","  ccdc_study_area = ccdcResult.reduceRegion(    \n","    reducer= ee.Reducer.toList(),    \n","    geometry = areaEstudo,    \n","    scale= 10)\n","  \n","  # converte o 'raster CCDC' em data frame, cada célula torna-se uma linha\n","  DF_study = pd.DataFrame(ccdc_study_area.getInfo())\n","  \n","  # adiciona as coordenadas do centróide de cada célula\n","  #df_Lat_Long = getLatLong(areaEstudo)\n","  dicLatLong = ee.Image.pixelLonLat().updateMask(ccdcResult.select('tStart').mask()).reduceRegion(\n","    reducer= ee.Reducer.toList(),\n","    geometry = areaEstudo,\n","    scale= 10)\n","  #converter para data frame\n","  df_Lat_Long = pd.DataFrame(dicLatLong.getInfo())\n","  # unir os dois DFs\n","  dataFrame = dataFrame.append(DF_study.join(df_Lat_Long), ignore_index = True)\n","  \n","  # Selecionar as colunas a explodir e as dos coeficientes\n","  tabExplode = []\n","  tabCoefs = []\n","  for c in dataFrame.columns:\n","    if 'coefs' in c or 'magnitude' in c or 'rmse' in c:\n","      tabExplode.append(c)\n","    if 'coefs' in c:\n","      tabCoefs.append(c)\n","  tabExplode = tabExplode + ['numObs','changeProb', 'tBreak', 'tEnd', 'tStart']\n","  \n","  dataFrame = dataFrame.explode(tabExplode)\n","  \n","  # Adicionar as colunas dos coeficientes dos breaks de cada banda\n","  coefsNames = [\"INTP\", \"SLP\", \"COS\", \"SIN\", \"COS2\", \"SIN2\", \"COS3\", \"SIN3\"]\n","  for col in tabCoefs:   \n","    prefixo = col.split('_')[0]\n","    colName = [prefixo + '_' + c for c in coefsNames]\n","    tempDf = pd.DataFrame(dataFrame[col].tolist(), columns=colName, index = dataFrame.index)\n","    dataFrame = pd.concat([dataFrame, tempDf], axis=1)  \n","\n","  dataFrame['End_S'] = endSerie\n","  dataFrame['coord_ccdc'] = list(zip(dataFrame.latitude, dataFrame.longitude))\n","  dataFrame['Dist_Point'] = ''\n","  dataFrame['Point_Val'] = '' \n","\n","  return dataFrame"],"metadata":{"id":"rBe4UoPpZNoH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Extrair para multiplas datas de fim"],"metadata":{"id":"09wZFp-3CY1W"}},{"cell_type":"code","source":["# #funcao bruno single point\n","\n","# def singlePoint(lat, long, nomeImage, date_start, end, indices, pointBuffer, bandas, bandas_breakpoint, bandas_tmask, minObs, chiSquare, minYears, dateForm, Lambda, maxIter):\n","#   #definir o fim do período no dicionario de parametros\n","#   params_ImgCol['date_end'] = end\n","#   # criar o ponto a analisar\n","#   ponto = ee.Feature(ee.Geometry.Point([long, lat]))\n","#   # dataframes temporarios a retornar\n","#   dfTemp = gpd.pd.DataFrame([[lat_ponto, long_ponto]], columns = ['lat','long'])\n","#   gdfTemp = gpd.GeoDataFrame(dfTemp, geometry= gpd.points_from_xy(dfTemp.long, dfTemp.lat))  \n","#   #Roda o ccdc e obtem as distancias\n","#   ImCol = funcoes.getImageCollection(params_ImgCol, ponto, pointBuffer)\n","#   ccdc = funcoes.runCCDC(ImCol, params_ImgCol['bandas'], params_ccdc)\n","#   df_VAL_Temp = funcoes.ccdcReducer(ponto, ccdc, pd.DataFrame(), end, pointBuffer)\n","#   gdfTemp, df_VAL_Temp = funcoes.addDist(gdfTemp, df_VAL_Temp, i )\n","#   # retorna o GDF do ponto unico com a distancia ao ponto mais proximo e o DF do CCDC com todas a distancia as este ponto\n","#   return gdfTemp, df_VAL_Temp"],"metadata":{"id":"PHx41mylCYS4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #funcao bruno multipoint\n","\n","# def multiPoint(end_dates, dfpoints, \n","#                df_ccdc, df_ccdc_agregado, banda, PATH, Outputs, CCDC_Data_Frame, Originais_Modelo, TestePontosAlt, # parametros para o nome do ficheiro\n","#                nomeImage, date_start, indices, pointBuffer, # parametros para a imagecollection\n","#                bandas, bandas_breakpoint, bandas_tmask, minObs, chiSquare, minYears, dateForm, Lambda, maxIter, # parametros CCDC\n","#                ):\n","#   #criar um dataframe vazio para ser preenchido a cada iteracao de fim de serie\n","#   df_total = pd.DataFrame()\n","\n","#   # Aqui precisamos verificar direito como estão nas funções\n","#   for end in end_dates:\n","#     #definir o fim do período no dicionario de parametros\n","#     params_ImgCol['date_end'] = end\n","#     i = 0\n","#     for Plat, Plon in list(dfpoints.col_lat_long):\n","#       prefixo = df_ccdc+'{0}_{1}_{2}.csv'.format(banda.upper(),dfpoints['id'].loc[i], end.replace('-',''))\n","#       ficheiro = os.path.join(PATH, Outputs, CCDC_Data_Frame, Originais_Modelo,TestePontosAlt,prefixo)\n","#       if os.path.exists(ficheiro):\n","#         # A ideia aqui seria caso os data frames existissem abrir e nao rodar o resto\n","#         df_VAL_CCDC = pd.read_csv(ficheiro, delimiter=';')\n","#         df_total = df_total.append(df_VAL_CCDC)\n","#       else:      \n","#         df_VAL_CCDC = pd.DataFrame() # Para o caso de salvar ponto a ponto\n","#         # cria o ponto\n","#         ponto = ee.Feature(ee.Geometry.Point([Plon,Plat]))\n","\n","#         ImCollection = funcoes.getImageCollection(params_ImgCol, ponto, pointBuffer)\n","\n","#         ccdc_result = funcoes.runCCDC(ImCollection, params_ImgCol['bandas'], params_ccdc)\n","        \n","#         df_VAL_CCDC = funcoes.ccdcReducer(ponto, cdc_result,df_VAL_CCDC, end, pointBuffer)\n","\n","#         dfpoints, df_VAL_CCDC = funcoes.addDist(dfpoints, df_VAL_CCDC, i )\n","      \n","#         # deixei a identação como para salvar um ponto de cada vez      \n","#         df_VAL_CCDC.to_csv(ficheiro ,sep=';')     \n","        \n","#         #append no df total\n","#         df_total = df_total.append(df_VAL_CCDC)\n","#     # Verifica a existencia do ficheiro completo\n","#     save_name = df_ccdc_agregad + \"{}.csv\".format(banda.upper())\n","#     ficheiroCompleto = os.path.join(PATH, Outputs, CCDC_Data_Frame, Originais_Modelo,TestePontosAlt,save_name)\n","#     # aqui a mesma coisa, caso exista abre, caso contrario o salva\n","#     if os.path.exists(ficheiroCompleto):\n","#       df_total = pd.read_cdv(ficheiroCompleto, delimiter = ';')      \n","#     else:\n","#       df_total.to_csv(ficheiroCompleto, sep = ';')\n","\n","#     return df_total"],"metadata":{"id":"VidUnLh3Cl4F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Função que cria o dataframe com a informação do CCDC. Execução demorada."],"metadata":{"id":"iIxDJBf0nDTI"}},{"cell_type":"code","source":["#funcao multiplos fins daniel\n","def extractInfoMultipleEndDates(end_dates, points_gdf, output_path, params_ImgCol, params_ccdc, pointBuffer):\n","\n","  \"\"\"Cria e guarda dataframes com informação do CCDC. Podem ser entradas múltiplas datas de\n","  fim de série.\n","  \n","  Entrada:\n","   - end_dates: lista contendo as datas de truncagem da serie do CCDC\n","   - points_gdf: data frame com as coordenadas dos pontos de analise\n","   - output_path: string com a pasta onde deseja salvar os ficheiros gerados\n","   - params_ImgCol: dicionario com os parametros para coletar a Image Collection\n","   - params_ccdc: dicionario com os parametros para gerar o CCDC\n","   - pointBuffer: inteiro com o valor do buffer ao redor do ponto\n","   Saida:\n","    - df_total: data frame com a informacao do ccdc para todos os pontos de analise  \n","  \"\"\"\n","\n","  #guardar date_end original\n","  original_date_end = params_ImgCol['date_end']\n","\n","  for end in end_dates:\n","    #criar um dataframe vazio para ser preenchido a cada iteracao de fim de serie\n","    df_total = pd.DataFrame() # Este DF precisa ser zerado a cada nova data pra nao ter duplicacoes\n","\n","    #atualizar o fim do período no dicionario de parametros\n","    params_ImgCol['date_end'] = end\n","\n","    #verifica se o ficheiro completo (pontos total) já existe. Caso exista, abre o ficheiro\n","\n","    save_name = \"Pontos_total_{}.csv\".format(fromParamsReturnName(params_ImgCol, params_ccdc, 9999, 9999, pointBuffer)).replace('LON999900000E_LAT999900000N_','')\n","    ficheiroCompleto = os.path.join(output_path,save_name)\n","    if os.path.exists(ficheiroCompleto):\n","      print('Dataframe file {} found in storage. Reading from drive'.format(save_name))\n","      df_total = pd.read_csv(ficheiroCompleto, delimiter = ';')\n","      continue\n","\n","    print('Creating dataframe...')\n","\n","    i = 0\n","    for Plat, Plon in list(points_gdf.col_lat_long):\n","      \n","      #check if file already exists\n","      filename = fromParamsReturnName(params_ImgCol, params_ccdc, Plon, Plat, pointBuffer)\n","      ficheiro = os.path.join(output_path, filename + '.csv')\n","\n","      if os.path.exists(ficheiro):\n","        # A ideia aqui seria caso os data frames existissem abrir e nao rodar o resto\n","        df_VAL_CCDC = pd.read_csv(ficheiro, delimiter=';')\n","        df_total = df_total.append(df_VAL_CCDC)\n","\n","      else:      \n","        df_VAL_CCDC = pd.DataFrame() # Para o caso de salvar ponto a ponto\n","        # cria o ponto     \n","        ponto = ee.Feature(ee.Geometry.Point([Plon,Plat]))\n","\n","        # Coleta o conjunto de Imagens Sentinel 2\n","        ImCollection = getImageCollection(params_ImgCol, ponto, buf=pointBuffer)\n","\n","        #run ccdc\n","        ccdc_result = runCCDC(ImCollection, params_ImgCol['bandas'], params_ccdc)\n","        \n","        # Extrai o CCDC para o Ponto ou Para o Raio ao Redor do Ponto - É esta funcao que demora de ser executada\n","        df_VAL_CCDC = ccdcReducer(pointGeom = ponto, ccdcResult = ccdc_result,\n","                                          dataFrame = df_VAL_CCDC, endSerie = end,\n","                                          buf = pointBuffer)       \n","\n","        # AQUI ENTRAR A FUNCAO NOVA DE DISTANCIA PARA TESTAR - aparentemente ok, testar com todos\n","        # converte o DF para GDF para poder utilizara as distancias entre as coordenadas\n","        # df_VAL_CCDC['coord_ccdc'] = list(zip(df_VAL_CCDC.latitude, df_VAL_CCDC.longitude))\n","        # df_VAL_CCDC['coord_ccdc'] = df_VAL_CCDC['coord_ccdc'].astype(str)\n","        # gdf_VAL_CCDC = gpd.GeoDataFrame(df_VAL_CCDC, geometry=gpd.points_from_xy( df_VAL_CCDC.longitude, df_VAL_CCDC.latitude), crs= 'EPSG: 4326' )\n","        # gdf_VAL_CCDC.to_crs(crs = 'EPSG: 3763', inplace = True)\n","\n","        # aplica as distancias aos DFs\n","        points_gdf, df_VAL_CCDC = addDist(points_gdf, df_VAL_CCDC, i)\n","        # gdf_VAL_CCDC_dist = ckdnearest(gdf_VAL_CCDC, points_gdf) #novo join\n","        \n","        # deixei a identação como para salvar um ponto de cada vez      \n","        #df_VAL_CCDC.to_csv(ficheiro ,sep=';') # salva um df para cada ponto\n","        \n","        #append no df total\n","        df_total = df_total.append(df_VAL_CCDC)\n","        # df_total = df_total.append(gdf_VAL_CCDC_dist) # novo join\n","\n","      i += 1\n","    \n","    # Verifica a existencia do ficheiro completo\n","    #save_name = \"Pontos_total_{}.csv\".format(fromParamsReturnName(params_ImgCol, params_ccdc, 9999, 9999, pointBuffer)).replace('LON999900000E_LAT999900000N_','')\n","    #ficheiroCompleto = os.path.join(output_path,save_name)\n","    # aqui a mesma coisa, caso exista abre, caso contrario o salva\n","    #if os.path.exists(ficheiroCompleto):\n","      #df_total = pd.read_csv(ficheiroCompleto, delimiter = ';')\n","      #pass\n","    #else:\n","      #df_total.to_csv(ficheiroCompleto, sep = ';')\n","    df_total.to_csv(ficheiroCompleto, sep = ';')\n","\n","  #restaurar date_end para o valor original\n","  params_ImgCol['date_end'] = original_date_end\n","  \n","  return df_total, ficheiroCompleto # Precisei colocar para retornar o nome do ficheiro criado também, se não perdemos essa info."],"metadata":{"id":"ixqD9-qDCprd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####convertDate (pequena função auxiliar)"],"metadata":{"id":"CMfzf9YHzuWz"}},{"cell_type":"code","source":["def convertDate(data):\n","  \"\"\"Obtem ano, mês e dia a partir de data no formato YYYY-MM-DD\"\"\"\n","  data = data.split('-')\n","  y = int(data[0])\n","  m = int(data[1])\n","  d = int(data[2])\n","  return y,m,d"],"metadata":{"id":"6c0C3RuV0EoJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Função para o Join Espacial entre o dataframe do CCDC e a informação vetorial da DGT"],"metadata":{"id":"FurBsye9e_0E"}},{"cell_type":"code","source":["def spatialJoin(pathPoligonosDGT, dfCCDC):\n","  \"\"\"\n","  Realizar o spatial join entre o dataframe do CCDC e os poligonos com alteracoes identificadas pela DGT\n","  Entrada:   \n","   - pathPoligonosDGT: String com o caminho completo dos poligonos desenhados pela DGT\n","   - pathDataFrameCCDC: Data Frame filtrado do CCDC\n","   Saida:\n","   \"\"\"\n","  # 1) ABRIR OS ARQUIVOS\n","  ## Poligonos DGT\n","  gdfVal = gpd.read_file(pathPoligonosDGT)\n","  gdfVal.to_crs(crs = 'EPSG: 3763', inplace = True) # Originalmente eles estao em WGS84 29N converte para ETRS\n","  ## Pontos ISA\n","  \n","  # 2) CONVERTER O DF PARA GEO DF\n","  gdfCCDC = gpd.GeoDataFrame(dfCCDC, geometry = gpd.points_from_xy(dfCCDC.longitude, dfCCDC.latitude), crs = 4326 )\n","   \n","  ## criar a bordadura\n","  ###idBord = identity.copy() # cria uma copia do identity gerado acima\n","  idBord = gdfVal.copy()\n","  idBord['geometry'] = idBord.geometry.buffer(-10) # reduz a geometria em 10 metros\n","  idBord.drop(list(idBord.columns)[:-1], axis = 1, inplace = True) #remove todas as colunas menos a da geometria\n","  idBord['bordadura'] = 1 # cria uma nova coluna para poder identificar a borda dura\n","  ## novo identity para termos a area da borda dura\n","  \n","  ###identity = gpd.overlay(identity, idBord, how='identity')\n","  identity = gpd.overlay(gdfVal, idBord, how = 'identity')\n","  \n","  # Como o poligono inicial nao tinha a coluna de bordadura, há feições onde\n","  # temos 1 e Nulos, com a linha abaixo invertemos o campo onde era Nullo passa a True\n","  # e onde era 1 passa para False, ou 1 e 0\n","  identity.bordadura = identity.bordadura.isnull() \n","  # Convertemos o resultado para WGS84\n","  identity.to_crs(crs = 'EPSG: 4326', inplace = True)\n","  ## As datas da DGT estao no formato (20200103) e precisam ser convertidas\n","  for dataCol in ['data_0', 'data_1', 'data_2', 'data_3']:      \n","      # primeiro converter para datetime\n","      maskZero = pd.Series(np.zeros(len(identity),dtype=bool))\n","      erro = identity[dataCol].isnull()\n","      identity.loc[erro, dataCol] = 0\n","      # converter tudo para inteiros e onde for 0 indicar 1970\n","      identity[dataCol] = identity[dataCol].astype(int)\n","      maskZero = identity.loc[:, dataCol] == 0\n","      identity.loc[maskZero, dataCol] = 19700101\n","      # converter para datetime  \n","      identity[dataCol] = pd.to_datetime(identity[dataCol], format = '%Y%m%d')\n","      identity.loc[maskZero, dataCol] = np.nan\n","\n","  # 4) SPATIAL JOIN ENTRE OS CENTROIDES DO CCDC COM OS BUFFERS DE 200 METROS\n","  subset = gpd.sjoin(gdfCCDC, identity, how='inner')\n","  subset.reset_index(inplace = True)\n","  subset['buffer_ID'] = subset.buffer_ID.astype('int')\n","    \n","  \"\"\"\n","  Descobrir quais linhas precisam ser duplicadas.\n","  Pressupondo que não é possível ter informação da 'data_3' sem existir a 'data_1'\n","  é possível filtrar e verificar a negação de quais dados são nulos e depois somar\n","  o reultado.\n","  0 = False False: não há data_1 e nem data_3\n","  1 = True  False: existe data_1 e não data_3\n","  2 = True  True: existem data_1 e Data_3  \n","  \"\"\"\n","  cond = ~subset.filter(items=['data_1', 'data_3']).isnull()\n","  subset['analistas'] = cond.sum(axis=1)\n","  subset.loc[subset['analistas'] == 0, 'exists_event'] = False # Analista nao identificou nada\n","  subset.loc[subset['analistas'] > 0, 'exists_event'] = True # Analista identificou alteracao\n","  \n","  \"\"\"\n","  CRIA UM DF TEMPORARIO PARA COPIAR AS LINHAS ONDE EXISTEM A 'DATA_3' E INSERE ESTA DATA NO CAMPO 'DATA1_Z'\n","  DEPOIS ADICIONA ISTO AO DATA FRAME ORIGINAL\n","  \"\"\" \n","  subset['data1_z'] = ''\n","  # criar coluna para as datas anteriores\n","  # subset['data0_z'] = '' \n","  subset['nome'] = '' # teste para nomear os analistas\n","  subset['tipo'] = ''\n","  subset['classeAnterior'] = '' \n","  subset['classeAtual'] = ''   \n","  dfTemp = pd.DataFrame(columns = subset.columns)\n","  for row in subset.itertuples():\n","    # verifica se há duas datas e duplica a linha\n","    if row.analistas == 2:\n","        dfTemp = dfTemp.append(subset[subset.index == row.Index], ignore_index=False) \n","  dfTemp.data1_z = dfTemp.data_3\n","  # capturar o valor da data_2\n","  # defTemp.data0_z = dfTemp.data_2\n","  dfTemp.nome = 'B' # teste para nomear os analistas\n","  dfTemp.tipo = dfTemp.tipo_2\n","  dfTemp.classeAtual = dfTemp.classe_3\n","  dfTemp.classeAnterior = dfTemp.classe_2\n","\n","  subset.data1_z = subset.data_1\n","  # capturar o valor da data_0\n","  # subset.data0_z = subset.data_0\n","  subset.nome = 'A' # teste para nomear os analistas\n","  subset.tipo = subset.tipo_1\n","  subset.classeAtual = subset.classe_1\n","  subset.classeAnterior = subset.classe_0\n","\n","  subset = subset.append(dfTemp, ignore_index=False)\n","\n","  # Contagem do numero de breaks\n","  subset['Valid_breaks'] = np.ceil(subset.groupby(['coord_ccdc', 'nome'])['changeProb'].transform('sum'))\n","\n","  # COLUNA DO DELTA MIN\n","  subset['delta_min'] = (subset.data1_z - subset.tBreak).dt.days\n","  subset.drop(['data_1', 'data_3', 'tipo_1', 'tipo_2','classe_0', 'classe_1','classe_2', 'classe_3'], axis = 1, inplace = True)  \n","\n","  # verificar quais colunas tem magnitude de indices\n","  mags = [ t for t in subset.columns if 'magnitude' in t and not 'B' in t] \n","  ordem = [ 'coord_ccdc','buffer_ID', 'IDCCDC', 'altera', 'changeProb'] + mags + ['tBreak', 'data1_z',\n","         'bordadura', 'classe2018', 'classe2019', 'classe2020','classe2021', 'classeAnterior','tipo',\n","         'classeAtual', 'analistas', 'nome', 'exists_event', 'Valid_breaks' , 'delta_min', 'geometry']\n","    \n","\n","  return subset[ordem], subset"],"metadata":{"id":"iCHy5mvLfMfm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Função de Validação "],"metadata":{"id":"n5z43-DGs9AR"}},{"cell_type":"code","source":["# função para realizar a limpeza de linhas indesejadas\n","def testeRemove(groupedby): \n","  min_delta_min = groupedby['Min_delta_min'].min()\n","  #remove rows only if there is more than 1 row per point, the number of analyst dates is not zero and min_delta_min is greater than zero.\n","  if len(groupedby) > 1 and groupedby.analistas.min() > 0 and min_delta_min > 0:\n","      Bj, Ai = groupedby.loc[groupedby['delta_min']==min_delta_min][['tBreak','data1_z']].values[0]\n","      #remove rows that contain Ai or Bj (other than the row with the min_delta_min)\n","      mask = ((groupedby['tBreak'] == Bj) | (groupedby['data1_z'] == Ai)) & (groupedby['delta_min']!=min_delta_min)\n","      groupedby = groupedby[~mask]\n","\n","  return groupedby\n","\n","# função de validação do data frame\n","def valPol(df, theta):\n","  \"\"\"\n","  Esta função recebe o geodataframe gerado no spatialJoin() e contabiliza as métricas de positivos e negativos.\n","  A Saída é a matriz com os cálculos e um dicinário com as métricas contabilizadas. \n","  \n","  \"\"\"\n","  \n","  # transforma a coluna de delta min para valor absoluto e cria uma nova coluna com o mínimo delta min por ponto\n","  df.reset_index(inplace = True)\n","  original_delta_min = df['delta_min'].copy()\n","  df['delta_min'] = abs(df['delta_min'].fillna(99999)) # substitui os nullos para evitar que sejam os minimos\n","  df['Min_delta_min'] = df.groupby(['coord_ccdc', 'nome'])['delta_min'].transform('min') # calcula o valor minimo por ponto\n","  df['delta_min'] = abs(original_delta_min) # retorna o valor absoluto da coluna original\n","  df['Min_delta_min'] = df['Min_delta_min'].replace(99999,np.nan) # substitui os 99999 por nullos\n","\n","  bf = df.copy()\n","\n","  bf['Valid_breaks'] = bf.groupby(['coord_ccdc', 'nome']).transform('count')[['tBreak']] # verifica os breaks validos por pontos\n","  # SE O TBREAK FOR OBJETO ELE JAMAIS SERA NULO, CONVERTER PARA DATA.\n","  bf.tBreak = pd.to_datetime(bf.tBreak)\n","  bf.tStart = pd.to_datetime(bf.tStart)\n","  bf.tEnd = pd.to_datetime(bf.tEnd)\n","  bf.analistas = bf.analistas.astype(int)\n","  bf.exists_event = bf.exists_event.astype(int)\n","  bf.buffer_ID = bf.buffer_ID.astype(int)\n","  bf.IDCCDC = bf.IDCCDC.astype(int)\n","\n","  ## ALGUMAS MASCARAS INICIAIS NECESSARIAS\n","  # mascara dos breaks a mais que analistas ainda em reformulacao\n","\n","  # PARA O CASO DE TER SOMENTE UM BREAK FP E DOIS ANALISTAS PARA NAO TER DUPLICACAO\n","  mask = pd.Series(np.zeros(len(bf),dtype=bool), index= bf.index)\n","  mask.loc[(bf.analistas == 2) & (bf.Valid_breaks < bf.analistas) ] = True #& (bf.delta_min > theta) \n","  \n","  bf.loc[mask, 'Min_delta_min'] = bf.loc[mask].groupby(['coord_ccdc'])['delta_min'].transform('min')\n","\n","  # Contabilizar \n","  # colocar todos os VP (delta_min <=31)\n","  #VP\n","  bf.loc[( (bf.delta_min <= theta) & (~bf.tBreak.isnull()) & (bf.analistas > 0) ), 'VP'] = 1\n","  # #FP\n","  # # sem a condição da magnitude ou (changeProb ==1) serao selecionados os que devem ser negativos\n","  # bf.loc[( (bf.analistas == 0) & (bf.ndvi_magnitude != 0) & (~bf.tBreak.isnull())), 'FP' ] = 1 #FP puro\n","  # bf.loc[( (bf.delta_min > theta) & (bf.ndvi_magnitude != 0) & ( (bf.delta_min == bf.Min_delta_min) & (~bf.Min_delta_min.isnull()) ) )  , 'FP' ] = 1\n","  # bf.loc[( (bf.delta_min > theta) & (bf.ndvi_magnitude != 0) & (bf.analistas == 1)  ) & (~bf.tBreak.isnull()), 'FP' ] = 1\n","  #FP\n","  # sem a condição da magnitude ou (changeProb ==1) serao selecionados os que devem ser negativos\n","  bf.loc[( (bf.analistas == 0) & (~bf.tBreak.isnull())), 'FP' ] = 1 #FP puro\n","  bf.loc[( (bf.delta_min > theta) & ( (bf.delta_min == bf.Min_delta_min) & (~bf.Min_delta_min.isnull()) ) )  , 'FP' ] = 1\n","  bf.loc[( (bf.delta_min > theta) & (bf.analistas == 1)  ) & (~bf.tBreak.isnull()), 'FP' ] = 1\n","  #FN\n","  bf.loc[( (bf.analistas > 0)  & (bf.tBreak.isnull()) ), 'FN' ] = 1 # FN puro\n","  # falsos negativos que precisam ser contabilizado para os FPs\n","  bf.loc[(bf.analistas == 1) & (bf.Valid_breaks == 1) & (bf.FP == 1), 'FN'] = 1 # parece funcionar\n","  bf.loc[(bf.analistas == 2) & (bf.Valid_breaks == 3) & (bf.FP == 1) , 'FN'] = 1\n","  \n","  #VN\n","  bf.loc[( (bf.analistas == 0) & (bf.tBreak.isnull()) ), 'VN' ] = 1\n","\n","  # converter os NaN para 0\n","  bf[['VP', 'FP', 'FN', 'VN']] = bf[['VP', 'FP', 'FN', 'VN']].fillna(0)\n","\n","  # verificar os breaks que nao foram classificados\n","  # para isso gero uma coluna total onde somo todas as metricas, as linhas onde ha 0 nao foram classificadas\n","  bf['total'] = bf.VP + bf.FP +bf.FN + bf.VN\n","  mask = pd.Series(np.zeros(len(bf),dtype=bool), index= bf.index) #mascara\n","  # agrupar por coordenada e t break, assim as somente os breaks que nao foram validados para nenhum analista terao valor 0\n","  mask.loc[(bf.groupby(['coord_ccdc','tBreak'])['total'].transform('sum')==0) & (bf.analistas == 2) & (bf.Valid_breaks > bf.analistas)] = True\n","  # neste grupo selecionado devo procurar aquele que tem menor distancia para um analista e classificar como FP\n","  mask2 = bf[mask].groupby(['coord_ccdc'])['delta_min'].transform('min') == bf.delta_min[mask]\n","  # agora classificar os candidatos que atendem as duas mascaras\n","  bf.loc[(mask & mask2), ['FP']] = 1\n","\n","  # Ajuste FN\n","  # se for na célula anterior isso contará para o total e a mascara anterior não será feita em alguns pontos onde deve ser feita\n","  bf.loc[((bf.FP ==1) & (bf.analistas == 1) & (bf.delta_min == bf.Min_delta_min) & (bf.Valid_breaks == 2))   , 'FN' ] = 1\n","  bf.loc[((bf.FP ==1) & (bf.analistas == 1) & (bf.delta_min == bf.Min_delta_min) & (bf.Valid_breaks == 3))   , 'FN' ] = 1 \n","  bf.loc[(bf.analistas == 2) & (bf.Valid_breaks == 1) & (bf.VP == 0), 'FN'] = 1\n","  bf.loc[(bf.analistas == 2) & (bf.Valid_breaks == 2) & (bf.FP == 1), 'FN'] = 1\n","\n","  # Bloco para corrigir o problema de quando as duas datas DGT estão mais próximas do mesmo break\n","  # listar as coordenadas que tem o problema com mesmo break classificado\n","  listCoord = list(bf.coord_ccdc[(bf.groupby(['coord_ccdc','tBreak'])['total'].transform('sum') == 0) & (bf.analistas == 2) & (bf.Valid_breaks == 2)])\n","  # dividir o data frame em dois para poder limpar as linhas com problema\n","  bf_filter = bf.loc[~bf.coord_ccdc.isin(listCoord)].copy()\n","  # limpeza\n","  bf_remove_lines = bf.loc[bf.coord_ccdc.isin(listCoord)].copy()\n","  # zerar todas as métricas para poder recalcular\n","  bf_remove_lines.loc[:, ['VP','VN','FP', 'FN']] = 0  \n","  bf_removed = bf_remove_lines.groupby(['buffer_ID','IDCCDC']).apply(testeRemove).copy() # função de remoção\n","  bf_removed = bf_removed.drop(columns=['buffer_ID','IDCCDC']).reset_index() # evitar problema de indece dup.\n","  # Agora teremos somente duas linhas por ponto que são obrigatóriamente FP ou VP\n","  #VP\n","  bf_removed.loc[( (bf_removed.delta_min <= theta) ), 'VP'] = 1\n","  #FP, FN\n","  bf_removed.loc[( (bf_removed.delta_min > theta) ), ['FP', 'FN']] = 1\n","  # unir os dois dfs novamente\n","  bf_final = bf_filter.append(bf_removed)\n","\n","  # remover aqueles que nao possuem metrica\n","  bf_final = bf_final[(bf_final.VP > 0) | (bf_final.FP > 0) | (bf_final.FN > 0) | (bf_final.VN > 0) ].copy()\n","  # remover aqueles que apresentam as classes especificas\n","  bf_final = bf_final[~(bf_final.tipo.isin(['Agricultura','Agua']))].copy()\n","  \n","  \n","  # verificar quais colunas tem magnitude de indices\n","  mags = [ t for t in bf_final.columns if 'magnitude' in t and not 'B' in t]\n","  # colunas para retornar um DF mais limpo\n","  c = ['buffer_ID', 'IDCCDC', 'coord_ccdc', 'changeProb'] + mags + ['tBreak',\n","       'data1_z', 'analistas', 'nome', 'exists_event', 'Valid_breaks', \n","       'delta_min', 'Min_delta_min', 'VP', 'FP', 'FN', 'VN'] #geometry\n","  # também poderá retornar o DF todo classificado, em processo.\n","  return bf_final[c], bf_final"],"metadata":{"id":"GEPPy-RItGGx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Extrair informacao do raster para o ponto"],"metadata":{"id":"qo9tuWmpKIGN"}},{"cell_type":"code","source":["def getRasterValue(gdf, Raster, label):\n","  \"\"\"\n","  Extrair os valores de um raster para o ponto\n","  Entrada - gdf: Objeto GeoDataFrame\n","          - Raster: string com o caminho completo do raster para extrair os valores (eg COSsim2018_H)\n","          - label: string para identificar os rasters\n","  Saida - GeoDataFrame com os valores dos rasters\n","  \"\"\"\n","  raster = rasterio.open(Raster) # abre o arquivo raster\n","  gdf_crs = gdf.crs # salvar a coordenada original\n","  \n","  # Verifica se o raster e o geodataframe estao na mesma coordenada\n","  if gdf.crs != raster:\n","    gdf = gdf.to_crs(raster.crs)\n","  else:\n","    pass\n","  \n","  # coletar os pares de coordenadas x, y \n","  coords = [(x,y) for x,y in zip(gdf.geometry.x, gdf.geometry.y)]\n","  gdf[label] = [x[0] for x in raster.sample(coords)]\n","  \n","  # Verificar as coordenadas de retorno\n","  if gdf.crs != gdf_crs:\n","    gdf = gdf.to_crs(gdf_crs)\n","    return gdf\n","  else:\n","    return gdf"],"metadata":{"id":"yK-H4d-tKRxM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Transformar em matriz binária"],"metadata":{"id":"aLoHNlP49tGC"}},{"cell_type":"code","source":["def dfBinario_v2(DF_FINAL):\n","  \"\"\"\n","  Converte a data frame para matriz binária\n","  Entrada: Data Frame completa com toda a informacao\n","  Saida: nova Data Frame com os valores binarios\n","   \n","  Classes de uso do solo indentificados pela DGT\n","  PBR: Pinheiro bravo; MAT: Matos; VHE: Vegetacao herbace espontanea; EUC: Eucalipto;\n","  OFP: Outras folhosas persistente; AGR: Agricultura; SVE: Superficie sem vegetacao escura;\n","  SVC: Superficie sem vegetacao clara; \n","  OFC: Outras folhosas caduca; SEA: Sobreiro e Azinheira; PMA: Pinheiro Manso\n","\n","  tipo de alteracao identificada pela DGT\n","  TIP_AGR:  Agricultura, TIP_AGU: Agua, TIP_FOG:Fogo, TIP_COR: Corte\n","\n","  Classes de uso do solo da COSSIM\n","  100: Artificializado; 200: Agricultura (somente COSSIM 18); 211: Culturas anuais de outono inverno;\n","  212: Culturas anuais de primavera verao; 213: Outras areas agrucilas; 311: Sobreiro e Azinheira;\n","  312: Eucalipto; 313: Outras folhosas; 321: Pinheiro bravo; 322: Pinheiro manso, 323: Outras reinosas;\n","  410: Matos; 420: Vegetacao herbacea espontanea; 500: Supercies sem vegetacao; 610: Zonas humidas;\n","  620: Agua\n","  \"\"\"\n","\n","  sub = DF_FINAL.copy()\n","  new = pd.DataFrame(index= sub.index)\n","\n","  dicTipo = {'Agricultura': 'TIP_AGR', 'Agua': 'TIP_AGU', 'Fogo': 'TIP_FOG', 'Corte': 'TIP_COR'}\n","\n","  dicCossim = {100: 'ART', 200: 'AGR', 211: 'AGR', 212: 'AGR', 213: 'AGR', 311: 'SEA', 312: 'EUC', 313: 'OFS',\n","          321: 'PBR', 322: 'PMA', 323: 'ORS', 410: 'MAT', 420: 'VHE', 500: 'SVG',610: 'ZHU', 620: 'AGU'}\n","\n","  dicClasse = {'Superficie sem vegetacao escura': 'SVE', 'Pinheiro bravo': 'PBR', 'Pinheiro manso': 'PBA',\n","              'Matos': 'MAT', 'Vegetacao herbacea espontanea': 'VHE', 'Eucalipto': 'EUC', 'Outras folhosas persistente': 'OFP',\n","              'Outras folhosas caduca': 'OFC', 'Sobreiro e Azinheira': 'SEA', 'Agricultura': 'AGR', 'Superficie sem vegetacao clara': 'SVC' }\n","\n","  # Converter as colunas de datas para numero de dias apos 01/01/2016.\n","  sub['tStart'] =  (sub.tStart - pd.to_datetime(datetime.datetime(2016,1,1))).dt.days # dara do inicio do segmento\n","  sub['breakMS'] = (sub.tBreak - pd.to_datetime(datetime.datetime(2016,1,1))).dt.days # data do break\n","  sub['tEnd'] = (sub.tEnd - pd.to_datetime(datetime.datetime(2016,1,1))).dt.days # data do final do segmento\n","  sub['dgtMS'] = (sub.data1_z - pd.to_datetime(datetime.datetime(2016,1,1))).dt.days # data da alteracao do analista\n","  #sub['dgtMS_ant'] = (sub.data0_z - pd.to_datetime(datetime.datetime(2016,1,1))).dt.days # data da imagem interior a alteracao do analista\n","\n","  sub.loc[sub['altera'] == 'Com Alteracao', 'altera'] = 1\n","  sub.loc[sub['altera'] == 'Sem Alteracao', 'altera'] = 0\n","\n","  for coluna in sub.columns:\n","    if 'tipo' in coluna: # converte os valores do 'TIPO' de ocorrencia identificada pela DGT para a alteracao\n","      for kt in dicTipo.keys():\n","        new[dicTipo[kt]] = 0\n","        new.loc[sub[coluna] == kt, dicTipo[kt]] = 1\n","\n","    if 'classe' in coluna: # converte os valores das 'CLASSES' de ocupacao indetificada pela DGT\n","      for kc in dicClasse.keys():\n","        novaColC = dicClasse[kc] + '_' + coluna.replace('classe', '')\n","        new[novaColC] = 0\n","        new.loc[sub[coluna] == kc, novaColC] = 1\n","\n","    if 'COSSIM' in coluna: # converte os valores das 'COSSIMs' originais\n","      for k in dicCossim.keys():\n","        novaCol = dicCossim[k] + '_' + coluna      \n","        new[novaCol] = 0\n","        new.loc[sub[coluna] == k, novaCol] = 1  \n","  # colunas desnecessarias na matriz binaria\n","  remove = ['buffer_ID', 'IDCCDC', 'altera', 'changeProb', 'tBreak', 'classe2018', 'classe2019', 'classe2020',\n","            'classe2021','coord_ccdc', 'data1_z', 'tipo', 'classeAnterior', 'classeAtual', 'Min_delta_min',\n","            'delta_min', 'Valid_breaks', 'nome', 'analistas', 'notas', 'area', 'data_2',\n","              'data_0', 'ID', 'index_right',  'breaks_in_tmask', 'numBreak', 'startMin', 'Point_Val',\n","              'Dist_Point', 'End_S', 'numObs', 'level_0', 'index', 'latitude', 'longitude', 'total', 'level_2']  \n","  for c in sub.columns: # verifica as colunas com cossim ou coef no nome para eliminar\n","    if '_coefs' in c or 'COSSIM' in c: \n","      remove.append(c)\n","\n","  subreturn = sub.drop(remove, axis=1) # Remove as colunas que serao desnecessarias\n","  #subreturn = sub[ordem].copy() # A parte da ordem pode ser utilizada para selecionar as colunas que queira se retornar\n","  subreturn.bordadura = subreturn.bordadura.astype(int)\n","  subreturn.exists_event = subreturn.exists_event.astype(int)\n","\n","  return pd.concat([subreturn,new], axis=1)"],"metadata":{"id":"VIjlLy5BB3Ea"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Função antiga, substituida pela anterior acima"],"metadata":{"id":"8kCvwL9ysRS6"}},{"cell_type":"code","source":["def dfBinario(sub):\n","  # Converter as colunas de datas para numero de dias apos 01/01/2016.\n","  sub['tStart'] =  (sub.tStart - pd.to_datetime(datetime.datetime(2016,1,1))).dt.days \n","  sub['breakMS'] = (sub.tBreak - pd.to_datetime(datetime.datetime(2016,1,1))).dt.days\n","  sub['tEnd'] = (sub.tEnd - pd.to_datetime(datetime.datetime(2016,1,1))).dt.days\n","  sub['dgtMS'] = (sub.data1_z - pd.to_datetime(datetime.datetime(2016,1,1))).dt.days\n","\n","  \"\"\"\n","  Classes de uso do solo indentificados\n","  PBR: Pinheiro bravo; MAT: Matos; VHE: Vegetacao herbace espontanea; EUC: Eucalipto;\n","  OFP: Outras folhosas persistente; AGR: Agricultura; SVE: Superficie sem vegetacao escura;\n","  SVC: Superficie sem vegetacao clara; \n","  OFC: Outras folhosas caduca; SEA: Sobreiro e Azinheira; PMA: Pinheiro Manso\n","\n","  tipo de alteracao identificada\n","  TIP_AGR:  Agricultura, TIP_AGU: Agua, TIP_FOG:Fogo, TIP_COR: Corte\n","  \"\"\"\n","  \n","  nCol = ['PBR', 'PMA', 'MAT', 'VHE', 'EUC', 'OFP','OFC', 'SEA', 'AGR', 'SVE', 'SVC']\n","  tipos = ['TIP_AGR', 'TIP_AGU', 'TIP_FOG', 'TIP_COR']\n","  cossims = ['ART', 'AGR', 'SEA', 'EUC', 'OFS','PBR', 'PMA', 'ORS', 'MAT', 'VHE', 'SVG', 'AGU']\n","  nColA = []\n","  n18 = []\n","  n19 = []\n","  n20 = []\n","  n21 = []\n","  cossim2018 = []\n","  cossim2020 = []\n","  cossim2021 = []\n","  for i in nCol:\n","    nColA.append(i+ '_ANT')\n","    n18.append(i + '_DGT2018')\n","    n19.append(i + '_DGT2019')\n","    n20.append(i + '_DGT2020')\n","    n21.append(i + '_DGT2021')\n","  for c in cossims:\n","    cossim2018.append(c + '_COSsim18_H')\n","    cossim2020.append(c + '_COSsim20_H')\n","    cossim2021.append(c + '_COSsim21_H')\n","\n","  sub[tipos] = 0\n","  sub[nColA] = 0\n","  sub[nCol] = 0\n","  sub[n18] = 0\n","  sub[n19] = 0\n","  sub[n20] = 0\n","  sub[n21] = 0\n","  sub[cossim2018]=0\n","  sub[cossim2020]=0\n","  sub[cossim2021]=0\n","\n","  sub.loc[sub['altera'] == 'Com Alteracao', 'altera'] = 1\n","  sub.loc[sub['altera'] == 'Sem Alteracao', 'altera'] = 0\n","\n","  sub.loc[sub['tipo'] == 'Agricultura', 'TIP_AGR'] = 1\n","  sub.loc[sub['tipo'] == 'Agua', 'TIP_AGU'] = 1\n","  sub.loc[sub['tipo'] == 'Fogo', 'TIP_FOG'] = 1\n","  sub.loc[sub['tipo'] == 'Corte', 'TIP_COR'] = 1\n","\n","  sub.loc[sub['classeAtual'] == 'Pinheiro bravo', 'PBR'] = 1\n","  sub.loc[sub['classeAtual'] == 'Pinheiro manso', 'PMA'] = 1\n","  sub.loc[sub['classeAtual'] == 'Matos', 'MAT'] = 1\n","  sub.loc[sub['classeAtual'] == 'Vegetacao herbacea espontanea', 'VHE'] = 1\n","  sub.loc[sub['classeAtual'] == 'Eucalipto', 'EUC'] = 1\n","  sub.loc[sub['classeAtual'] == 'Outras folhosas persistente', 'OFP'] = 1\n","  sub.loc[sub['classeAtual'] == 'Agricultura', 'AGR'] = 1\n","  sub.loc[sub['classeAtual'] == 'Superficie sem vegetacao escura', 'SVE'] = 1\n","  sub.loc[sub['classeAtual'] == 'Superficie sem vegetacao clara', 'SVC'] = 1\n","  sub.loc[sub['classeAtual'] == 'Outras folhosas persistente', 'OFC'] = 1\n","  sub.loc[sub['classeAtual'] == 'Sobreiro e Azinheira', 'SEA'] = 1\n","\n","  sub.loc[sub['classeAnterior'] == 'Pinheiro bravo', 'PBR_ANT'] = 1\n","  sub.loc[sub['classeAnterior'] == 'Pinheiro manso', 'PMA_ANT'] = 1\n","  sub.loc[sub['classeAnterior'] == 'Matos', 'MAT_ANT'] = 1\n","  sub.loc[sub['classeAnterior'] == 'Vegetacao herbacea espontanea', 'VHE_ANT'] = 1\n","  sub.loc[sub['classeAnterior'] == 'Eucalipto', 'EUC_ANT'] = 1\n","  sub.loc[sub['classeAnterior'] == 'Outras folhosas persistente', 'OFP_ANT'] = 1\n","  sub.loc[sub['classeAnterior'] == 'Agricultura', 'AGR_ANT'] = 1\n","  sub.loc[sub['classeAnterior'] == 'Superficie sem vegetacao escura', 'SVE_ANT'] = 1\n","  sub.loc[sub['classeAnterior'] == 'Superficie sem vegetacao clara', 'SVC_ANT'] = 1\n","  sub.loc[sub['classeAnterior'] == 'Outras folhosas persistente', 'OFC_ANT'] = 1\n","  sub.loc[sub['classeAnterior'] == 'Sobreiro e Azinheira', 'SEA_ANT'] = 1\n","\n","  sub.loc[sub['classe2018'] == 'Pinheiro bravo', 'PBR_DGT2018'] = 1\n","  sub.loc[sub['classe2018'] == 'Pinheiro manso', 'PMA_DGT2018'] = 1\n","  sub.loc[sub['classe2018'] == 'Matos', 'MAT_DGT2018'] = 1\n","  sub.loc[sub['classe2018'] == 'Vegetacao herbacea espontanea', 'VHE_DGT2018'] = 1\n","  sub.loc[sub['classe2018'] == 'Eucalipto', 'EUC_DGT2018'] = 1\n","  sub.loc[sub['classe2018'] == 'Outras folhosas persistente', 'OFP_DGT2018'] = 1\n","  sub.loc[sub['classe2018'] == 'Agricultura', 'AGR_DGT2018'] = 1\n","  sub.loc[sub['classe2018'] == 'Superficie sem vegetacao escura', 'SVE_DGT2018'] = 1\n","  sub.loc[sub['classe2018'] == 'Superficie sem vegetacao clara', 'SVC_DGT2018'] = 1\n","  sub.loc[sub['classe2018'] == 'Outras folhosas persistente', 'OFC_DGT2018'] = 1\n","  sub.loc[sub['classe2018'] == 'Sobreiro e Azinheira', 'SEA_DGT2018'] = 1\n","\n","  sub.loc[sub['classe2019'] == 'Pinheiro bravo', 'PBR_DGT2019'] = 1\n","  sub.loc[sub['classe2019'] == 'Pinheiro manso', 'PMA_DGT2019'] = 1\n","  sub.loc[sub['classe2019'] == 'Matos', 'MAT_DGT2019'] = 1\n","  sub.loc[sub['classe2019'] == 'Vegetacao herbacea espontanea', 'VHE_DGT2019'] = 1\n","  sub.loc[sub['classe2019'] == 'Eucalipto', 'EUC_DGT2019'] = 1\n","  sub.loc[sub['classe2019'] == 'Outras folhosas persistente', 'OFP_DGT2019'] = 1\n","  sub.loc[sub['classe2019'] == 'Agricultura', 'AGR_DGT2019'] = 1\n","  sub.loc[sub['classe2019'] == 'Superficie sem vegetacao escura', 'SVE_DGT2019'] = 1\n","  sub.loc[sub['classe2019'] == 'Superficie sem vegetacao clara', 'SVC_DGT2019'] = 1\n","  sub.loc[sub['classe2019'] == 'Outras folhosas persistente', 'OFC_DGT2019'] = 1\n","  sub.loc[sub['classe2019'] == 'Sobreiro e Azinheira', 'SEA_DGT2019'] = 1\n","\n","  sub.loc[sub['classe2020'] == 'Pinheiro bravo', 'PBR_DGT2020'] = 1\n","  sub.loc[sub['classe2020'] == 'Pinheiro manso', 'PMA_DGT2020'] = 1\n","  sub.loc[sub['classe2020'] == 'Matos', 'MAT_DGT2020'] = 1\n","  sub.loc[sub['classe2020'] == 'Vegetacao herbacea espontanea', 'VHE_DGT2020'] = 1\n","  sub.loc[sub['classe2020'] == 'Eucalipto', 'EUC_DGT2020'] = 1\n","  sub.loc[sub['classe2020'] == 'Outras folhosas persistente', 'OFP_DGT2020'] = 1\n","  sub.loc[sub['classe2020'] == 'Agricultura', 'AGR_DGT2020'] = 1\n","  sub.loc[sub['classe2020'] == 'Superficie sem vegetacao escura', 'SVE_DGT2020'] = 1\n","  sub.loc[sub['classe2020'] == 'Superficie sem vegetacao clara', 'SVC_DGT2020'] = 1\n","  sub.loc[sub['classe2020'] == 'Outras folhosas persistente', 'OFC_DGT2020'] = 1\n","  sub.loc[sub['classe2020'] == 'Sobreiro e Azinheira', 'SEA_DGT2020'] = 1\n","\n","  sub.loc[sub['classe2021'] == 'Pinheiro bravo', 'PBR_DGT2021'] = 1\n","  sub.loc[sub['classe2021'] == 'Pinheiro manso', 'PMA_DGT2021'] = 1\n","  sub.loc[sub['classe2021'] == 'Matos', 'MAT_DGT2021'] = 1\n","  sub.loc[sub['classe2021'] == 'Vegetacao herbacea espontanea', 'VHE_DGT2021'] = 1\n","  sub.loc[sub['classe2021'] == 'Eucalipto', 'EUC_DGT2021'] = 1\n","  sub.loc[sub['classe2021'] == 'Outras folhosas persistente', 'OFP_DGT2021'] = 1\n","  sub.loc[sub['classe2021'] == 'Agricultura', 'AGR_DGT2021'] = 1\n","  sub.loc[sub['classe2021'] == 'Superficie sem vegetacao escura', 'SVE_DGT2021'] = 1\n","  sub.loc[sub['classe2021'] == 'Superficie sem vegetacao clara', 'SVC_DGT2021'] = 1\n","  sub.loc[sub['classe2021'] == 'Outras folhosas persistente', 'OFC_DGT2021'] = 1\n","  sub.loc[sub['classe2021'] == 'Sobreiro e Azinheira', 'SEA_DGT2021'] = 1\n","\n","  sub.loc[sub['COSSIM18_H'] == 100, 'ART_COSsim18_H'] = 1\n","  sub.loc[sub['COSSIM18_H'] == 200, 'AGR_COSsim18_H'] = 1\n","  sub.loc[sub['COSSIM18_H'] == 311, 'SEA_COSsim18_H'] = 1\n","  sub.loc[sub['COSSIM18_H'] == 312, 'EUC_COSsim18_H'] = 1\n","  sub.loc[sub['COSSIM18_H'] == 313, 'OFS_COSsim18_H'] = 1\n","  sub.loc[sub['COSSIM18_H'] == 321, 'PBR_COSsim18_H'] = 1\n","  sub.loc[sub['COSSIM18_H'] == 322, 'PMA_COSsim18_H'] = 1\n","  sub.loc[sub['COSSIM18_H'] == 323, 'ORS_COSsim18_H'] = 1\n","  sub.loc[sub['COSSIM18_H'] == 410, 'MAT_COSsim18_H'] = 1\n","  sub.loc[sub['COSSIM18_H'] == 420, 'VHE_COSsim18_H'] = 1\n","  sub.loc[sub['COSSIM18_H'] == 500, 'SVG_COSsim18_H'] = 1\n","  sub.loc[sub['COSSIM18_H'] == 620, 'AGU_COSsim18_H'] = 1\n","  \n","  sub.loc[sub['COSSIM20_H'] == 100, 'ART_COSsim20_H'] = 1\n","  sub.loc[sub['COSSIM20_H'] // 100 == 2, 'AGR_COSsim20_H'] = 1\n","  sub.loc[sub['COSSIM20_H'] == 311, 'SEA_COSsim20_H'] = 1\n","  sub.loc[sub['COSSIM20_H'] == 312, 'EUC_COSsim20_H'] = 1\n","  sub.loc[sub['COSSIM20_H'] == 313, 'OFS_COSsim20_H'] = 1\n","  sub.loc[sub['COSSIM20_H'] == 321, 'PBR_COSsim20_H'] = 1\n","  sub.loc[sub['COSSIM20_H'] == 322, 'PMA_COSsim20_H'] = 1\n","  sub.loc[sub['COSSIM20_H'] == 323, 'ORS_COSsim20_H'] = 1\n","  sub.loc[sub['COSSIM20_H'] == 410, 'MAT_COSsim20_H'] = 1\n","  sub.loc[sub['COSSIM20_H'] == 420, 'VHE_COSsim20_H'] = 1\n","  sub.loc[sub['COSSIM20_H'] == 500, 'SVG_COSsim20_H'] = 1\n","  sub.loc[sub['COSSIM20_H'] == 620, 'AGU_COSsim20_H'] = 1\n"," \n","  sub.loc[sub['COSSIM21_H'] == 100, 'ART_COSsim21_H'] = 1\n","  sub.loc[sub['COSSIM21_H'] // 100 == 2, 'AGR_COSsim21_H'] = 1\n","  sub.loc[sub['COSSIM21_H'] == 311, 'SEA_COSsim21_H'] = 1\n","  sub.loc[sub['COSSIM21_H'] == 312, 'EUC_COSsim21_H'] = 1\n","  sub.loc[sub['COSSIM21_H'] == 313, 'OFS_COSsim21_H'] = 1\n","  sub.loc[sub['COSSIM21_H'] == 321, 'PBR_COSsim21_H'] = 1\n","  sub.loc[sub['COSSIM21_H'] == 322, 'PMA_COSsim21_H'] = 1\n","  sub.loc[sub['COSSIM21_H'] == 323, 'ORS_COSsim21_H'] = 1\n","  sub.loc[sub['COSSIM21_H'] == 410, 'MAT_COSsim21_H'] = 1\n","  sub.loc[sub['COSSIM21_H'] == 420, 'VHE_COSsim21_H'] = 1\n","  sub.loc[sub['COSSIM21_H'] == 500, 'SVG_COSsim21_H'] = 1\n","  sub.loc[sub['COSSIM21_H'] == 620, 'AGU_COSsim21_H'] = 1\n","  \n","  remove = ['buffer_ID', 'IDCCDC', 'altera', 'changeProb', 'tBreak', 'classe2018', 'classe2019', 'classe2020',\n","          'classe2021','coord_ccdc', 'data1_z', 'tipo', 'classeAnterior', 'classeAtual', 'Min_delta_min',\n","           'delta_min', 'Valid_breaks', 'nome', 'analistas', 'notas', 'area', 'data_2',\n","            'data_0', 'ID', 'index_right',  'breaks_in_tmask', 'numBreak', 'startMin', 'Point_Val',\n","            'Dist_Point', 'End_S', 'numObs', 'level_0', 'index', 'latitude', 'longitude', 'total', 'level_2']  \n","  for c in sub.columns:\n","    if '_coefs' in c:\n","      remove.append(c)\n","\n","#   ordem = ['breakMS', 'tStart', 'tEnd', 'dgtMS', 'VP', 'FP', 'FN', 'VN', 'bordadura', 'exists_event', 'B11_COS', 'B11_COS2', 'B11_COS3',\n","#  'B11_INTP', 'B11_SIN', 'B11_SIN2', 'B11_SIN3', 'B11_SLP', 'B11_magnitude', 'B11_rmse', 'B12_COS', 'B12_COS2', 'B12_COS3',\n","#  'B12_INTP', 'B12_SIN', 'B12_SIN2', 'B12_SIN3', 'B12_SLP', 'B12_magnitude', 'B12_rmse','B2_COS', 'B2_COS2', 'B2_COS3', 'B2_INTP',\n","#  'B2_SIN', 'B2_SIN2', 'B2_SIN3', 'B2_SLP', 'B2_magnitude', 'B2_rmse','B3_COS', 'B3_COS2', 'B3_COS3', 'B3_INTP', 'B3_SIN',\n","#  'B3_SIN2', 'B3_SIN3', 'B3_SLP', 'B3_magnitude', 'B3_rmse', 'B4_COS', 'B4_COS2', 'B4_COS3', 'B4_INTP', 'B4_SIN', 'B4_SIN2',\n","#  'B4_SIN3', 'B4_SLP', 'B4_magnitude', 'B4_rmse', 'B8_COS', 'B8_COS2', 'B8_COS3', 'B8_INTP', 'B8_SIN', 'B8_SIN2', 'B8_SIN3',\n","#  'B8_SLP', 'B8_magnitude', 'B8_rmse','ndvi_COS', 'ndvi_COS2', 'ndvi_COS3', 'ndvi_INTP', 'ndvi_SIN', 'ndvi_SIN2', 'ndvi_SIN3',\n","#  'ndvi_SLP', 'ndvi_magnitude', 'ndvi_rmse', 'TIP_AGR', 'TIP_AGU', 'TIP_FOG', 'TIP_COR', 'PBR_ANT', 'MAT_ANT', 'VHE_ANT', 'EUC_ANT',\n","#  'OFP_ANT', 'OFC_ANT', 'SEA_ANT', 'AGR_ANT', 'SVE_ANT', 'SVC_ANT', 'PBR', 'MAT', 'VHE', 'EUC', 'OFP', 'OFC', 'SEA', 'AGR', 'SVE',\n","#  'SVC', 'PBR_DGT2018', 'PMA_DGT2018', 'MAT_DGT2018', 'VHE_DGT2018', 'EUC_DGT2018', 'OFP_DGT2018', 'OFC_DGT2018', 'SEA_DGT2018', 'AGR_DGT2018', 'SVE_DGT2018', 'SVC_DGT2018',\n","#  'PBR_DGT2019', 'PMA_DGT2019', 'MAT_DGT2019', 'VHE_DGT2019', 'EUC_DGT2019', 'OFP_DGT2019', 'OFC_DGT2019', 'SEA_DGT2019', 'AGR_DGT2019', 'SVE_DGT2019', 'SVC_DGT2019',\n","#  'PBR_DGT2020','PMA_DGT2020', 'MAT_DGT2020', 'VHE_DGT2020', 'EUC_DGT2020', 'OFP_DGT2020', 'OFC_DGT2020', 'SEA_DGT2020', 'AGR_DGT2020', 'SVE_DGT2020', 'SVC_DGT2020',\n","#  'PBR_DGT2021', 'PMA_DGT2021', 'MAT_DGT2021', 'VHE_DGT2021', 'EUC_DGT2021', 'OFP_DGT2021', 'OFC_DGT2021', 'SEA_DGT2021', 'AGR_DGT2021', 'SVE_DGT2021', 'SVC_DGT2021',\n","#  'ART_COSsim18_H', 'AGR_COSsim18_H', 'SEA_COSsim18_H', 'EUC_COSsim18_H', 'OFS_COSsim18_H', 'PBR_COSsim18_H', 'PMA_COSsim18_H', 'ORS_COSsim18_H', 'MAT_COSsim18_H',\n","#  'VHE_COSsim18_H', 'SVG_COSsim18_H', 'AGU_COSsim18_H', 'ART_COSsim20_H', 'AGR_COSsim20_H', 'SEA_COSsim20_H', 'EUC_COSsim20_H', 'OFS_COSsim20_H', 'PBR_COSsim20_H',\n","#  'PMA_COSsim20_H', 'ORS_COSsim20_H', 'MAT_COSsim20_H', 'VHE_COSsim20_H', 'SVG_COSsim20_H', 'AGU_COSsim20_H', 'ART_COSsim21_H', 'AGR_COSsim21_H', 'SEA_COSsim21_H',\n","#  'EUC_COSsim21_H', 'OFS_COSsim21_H', 'PBR_COSsim21_H', 'PMA_COSsim21_H', 'ORS_COSsim21_H', 'MAT_COSsim21_H', 'VHE_COSsim21_H', 'SVG_COSsim21_H', 'AGU_COSsim21_H',\n","#   'geometry']\n","\n","  subreturn = sub.drop(remove, axis=1)\n","  #subreturn = sub[ordem].copy()\n","  subreturn.bordadura = subreturn.bordadura.astype(int)\n","  subreturn.exists_event = subreturn.exists_event.astype(int)\n","\n","  return subreturn"],"metadata":{"id":"2Ej_LIAf9swx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####filtroDF - <font color='red'>ANTIGO</font>"],"metadata":{"id":"j9sYzxEf0eFF"}},{"cell_type":"code","source":["def filtroDF(validacao, DF, ti, tf):# Nao sera mais utilizada\n","  \"\"\"\n","  Filtra o data frame do ccdc para o periodo de analise desejado, eliminando\n","  os tbreaks fora do intervalo desejado. E adiciona a informacao do ccdc nos\n","  dados de validacao, a partir de um join espacial para o ponto mais proximo\n","  Entrada:\n","   - validacao: data frame com os pontos da analise\n","   - DF: data frame com o resultado do ccdc\n","   - ti: string com a data inicial do periodo de analise no formato 'YYYY-MM-DD'\n","   - tf: string com a data final do periodo de analise no formato 'YYYY-MM-DD'\n","  Saida:\n","   - join: data frame dos pontos de analise contendo a informacao do CCDC\n","  \"\"\"\n","  \n","  colunasFiltro = ['altrc1_z', 'tipo1_z', 'data1_z','GC_z', 'OBID', 'COS2018_Lg', 'ndvi_magnitude',\n","                   'ndvi_rmse','tBreak', 'tEnd', 'tStart', 'geometry', 'End_S' , 'coord_ccdc']\n","  \n","  # Transformar as colunas em Date Time\n","  for dtCol in DF.columns:\n","    if 'tBreak' in dtCol or 'tEnd' in dtCol or 'tStart' in dtCol:\n","      mask = DF.loc[:, dtCol] == 0\n","      DF[dtCol] = pd.to_datetime(DF[dtCol], unit = 'ms')\n","      DF.loc[mask, dtCol] = np.nan\n","    elif 'End_S' in dtCol:\n","      DF[dtCol] = pd.to_datetime(DF[dtCol])\n","  # filtro das datas\n","  yi, mi, di = convertDate(ti)\n","  fltInicial = datetime.datetime(yi, mi, di)\n","  yf, mf, df = convertDate(tf)\n","  fltFinal = datetime.datetime(yf, mf, df)\n","  if fltFinal > datetime.datetime(2020,7,28):\n","    fltFinal = datetime.datetime(2020,7,28)\n","\n","  mask = pd.Series(np.zeros(len(DF),dtype=bool))\n","  for dtCol in DF.columns:\n","    if 'tBreak' in dtCol:\n","      # esta condicao garante que somente as datas entre o intervalo selecionado não serão null\n","      cond=np.array(DF[dtCol] >= fltInicial) & np.array(DF[dtCol] <= fltFinal)\n","      mask.loc[cond] = True\n","      DF.loc[~mask, dtCol] = np.nan\n","  \n","  join = pd.merge(validacao, DF[(DF['tBreak'] >= fltInicial) & (DF['tBreak'] <= fltFinal)], on= 'coord_ccdc', how='left')# OBID era no local do geometry\n","  for c in join.columns:\n","    if c not in colunasFiltro:\n","      join.drop(c, axis = 1, inplace=True)\n","  \n","  if 'data1_z' in join.columns:\n","    # AQUI APLICA O FILTRO DE DATAS TAMBÉM PARA O Z, CASO O CONTRARIO TEREMOS MAIS FALSOS NEGATIVOS\n","    # primeiro converter para datetime\n","    maskZero = pd.Series(np.zeros(len(join),dtype=bool))\n","    erro = join['data1_z'].isnull()\n","    join.loc[erro, 'data1_z'] = 0\n","    # converter tudo para inteiros e onde for 0 indicar 1970\n","    join['data1_z'] = join['data1_z'].astype(int)\n","    maskZero = join.loc[:, 'data1_z'] == 0\n","    join.loc[maskZero, 'data1_z'] = 19700101\n","    # converter para datetime  \n","    join['data1_z'] = pd.to_datetime(join['data1_z'], format = '%Y%m%d')\n","    join.loc[maskZero, 'data1_z'] = np.nan\n","    # aplicar o filtro\n","    cond = np.array(join['data1_z'] >= fltInicial) & np.array(join['data1_z'] <= fltFinal)\n","    maskTime = pd.Series(np.zeros(len(join),dtype=bool))\n","    maskTime.loc[cond] = True\n","    join.loc[~maskTime, 'data1_z'] = np.nan\n","  \n","  return join \n","  "],"metadata":{"id":"ZGMKtIFMb9Ta"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Funções para validação - <font color='red'>ANTIGAS</font>"],"metadata":{"id":"U79FQZ1OOpJn"}},{"cell_type":"code","source":["#NAO UTILIZADAS\n","# the input is a Series of deltatimes, that can have positive or negative values\n","def extreme_deltatimes(dtseries,mytype):\n","  dtseries = dtseries[dtseries.notnull()] # remove not a time NaT\n","  dts = dtseries.dt.days.unique() # returns numpy array\n","  if mytype == 'min':\n","    return dts[np.argmin(abs(dts))] # deltamin  \n","\n","# myseries(Series) is one row of a DataFrame \n","# returns either deltamin(integer) or deltamax(integer) of myseries\n","def mydeltas(myseries, tbreakcolumns, magcolumns,eventcolumns,mytype):\n","  if myseries['breaks_in_tmask'] and myseries['exists_event']:   \n","    tBreakCol = tbreakcolumns[0]    \n","    tBreak = myseries.loc[tBreakCol]     \n","    return extreme_deltatimes(pd.to_datetime(myseries[eventcolumns]) - pd.to_datetime(tBreak),mytype)    \n","  else:\n","    return np.nan # if no break exists in the temporal filter\n","\n","def MAG_colname(tBreak_colname,INDEX):\n","  if isinstance(tBreak_colname,type(None)):\n","    return None\n","  else:\n","    s = tBreak_colname.split('tBreak')\n","    newCol = s[0]+INDEX+'_magnitude'\n","    return newCol\n","\n","def tBreak_colname(MAG_colname,INDEX):\n","  if isinstance(MAG_colname,type(None)):\n","    return None\n","  else:\n","    s = MAG_colname.split(INDEX+'magnitude')\n","    newCol = s[0]+'_tBreak'\n","    return newCol\n","\n","def convertDataFrame(df):\n","  \"\"\"Organizar o data frame para obtencao das metricas almejadas\"\"\"\n","  colNames = ['OBID','tBreak','ndvi_magnitude','breaks_in_tmask','exists_event',\n","              'num_events','delta_min', 'data1_z', 'tipo1_z']\n","  tbreakcolumns=[]\n","  magcolumns=[]\n","  # Criar as colunas com datas\n","  for dtCol in df.columns:\n","    if 'tBreak' in dtCol or 'tEnd' in dtCol or 'tStart' in dtCol or 'data1_z' in dtCol:\n","      mask = df.loc[:, dtCol] == 0\n","      df[dtCol] = pd.to_datetime(df[dtCol])\n","      df.loc[mask, dtCol] = np.nan  \n","    if 'tBreak' in dtCol: \n","      if df[dtCol].count() > 0 :\n","        newCol = MAG_colname(dtCol,INDEX)\n","        tbreakcolumns.append(dtCol) # only keep columns which have non nan's\n","        magcolumns.append(newCol) # columns _MAG that correspond to breaks  \n","  mask=pd.Series(np.zeros(len(df),dtype=bool)) # initialize array of False\n","  num_tbreaks_por_ponto = pd.Series(np.zeros(len(df))) # initialize array of zeros\n","\n","  for dtCol in tbreakcolumns:\n","    cond = ~np.isnan(np.array(df[dtCol]))\n","    mask.loc[cond] = True  \n","    num_tbreaks_por_ponto[cond] += 1\n","  \n","  df['breaks_in_tmask'] = mask\n","  df['num_breaks_in_tmask'] = pd.to_numeric(num_tbreaks_por_ponto,downcast='integer')\n","  \n","  eventcolumns = ['data1_z'] # somente contabilizar o Z\n","  mask = pd.Series(np.zeros(len(df),dtype=bool)) # number rows)\n","  num_events = pd.Series(np.zeros(len(df)))\n","  for dtCol in eventcolumns:\n","    cond = ~np.isnat(np.array(df[dtCol])) # ~ is negation element-wise\n","    mask.loc[cond] = True\n","    num_events[cond] += 1\n","  df['exists_event'] = mask\n","  df['num_events'] = num_events\n","  df['delta_min'] = df.apply(lambda row : mydeltas(row,tbreakcolumns, magcolumns,eventcolumns,'min'), axis = 1)\n","  \n","  return df[colNames].loc[:]\n","\n","# UTILIZAÇÃO: teste = convertDataFrame(gdf)"],"metadata":{"id":"cGlOOuJMOtuo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NAO UTILIZADAS\n","def validacaoDataFrame(df, theta = 30):\n","  \"\"\"\n","  Entrada: df: Data Frame filtrado para as datas desejadas\n","           theta: valor maximo de dias permitidos entre\n","           o modelo e o analista, default 30.\n","  Saida: Data Frames de VP, FP, FN e VN e matriz de confusao\n","  \"\"\"\n","  colNames=['OBID','tBreak','ndvi_magnitude','exists_event','num_events',\n","            'delta_min', 'breaks_in_tmask','data1_z', 'tipo1_z']\n","  #e[1] = Z True e break True e teta < +- theta dias\n","  VP = df[colNames].loc[(df.breaks_in_tmask == True) & (df.exists_event==True) & (abs(df.delta_min) <= theta)]\n","  mask=pd.Series(np.zeros(len(df),dtype=bool))\n","  mask.loc[(df.breaks_in_tmask == True) & (df.exists_event==True) & (abs(df.delta_min) <= theta)]=True\n","  df.loc[mask, 'CCDC_data'] = 1\n","  df.loc[mask, 'REF_DATA'] = 1\n","\n","  #e[2] = Z Falso e break = True\n","  FP = df[colNames].loc[(df.breaks_in_tmask == True) & (df.exists_event==False)]\n","  mask = pd.Series(np.zeros(len(df),dtype=bool))\n","  mask.loc[(df.breaks_in_tmask == True) & (df.exists_event==False)]=True\n","  df.loc[mask, 'CCDC_data'] = 1\n","  df.loc[mask, 'REF_DATA'] = 0\n","\n","  #e[3] = Z True e break False (pode ser fora do tmask ou nunca ter existido ou > +/- theta) \n","  # Apesar de o Modelo encontrar a quebra, como o teta é maior que o estipulado é como se o modelo tivesse errado e não encontrado a alteração\n","  FN = df[colNames].loc[((df.breaks_in_tmask == False) & (df.exists_event==True)) | ((df.breaks_in_tmask == True) & (abs(df.delta_min) > theta))]\n","  mask = pd.Series(np.zeros(len(df),dtype=bool))\n","  mask.loc[((df.breaks_in_tmask == False) & (df.exists_event==True)) | ((df.breaks_in_tmask == True) & (abs(df.delta_min) > theta))]=True\n","  df.loc[mask, 'CCDC_data'] = 0\n","  df.loc[mask, 'REF_DATA'] = 1\n","\n","  #e[4] = Z False e break False\n","  VN = df[colNames].loc[(df.breaks_in_tmask == False) & (df.exists_event==False)]\n","  mask=pd.Series(np.zeros(len(df),dtype=bool))\n","  mask.loc[(df.breaks_in_tmask == False) & (df.exists_event==False)]=True\n","  df.loc[mask, 'CCDC_data'] = 0\n","  df.loc[mask, 'REF_DATA'] = 0\n","\n","  confusion_matrix = pd.crosstab(df['REF_DATA'], df['CCDC_data'], rownames=['REFERENCIA'], colnames=['MODELO'])\n","\n","  return VP, FP, FN, VN, confusion_matrix\n","\n","\n","# UTILIZAÇÃO: vp, fp, fn, vn, matriz = validacaoDataFrame(teste, 30)"],"metadata":{"id":"BTkd-_gkQYyw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Adiciona as novas colunas para o Join e converte para WGS 84"],"metadata":{"id":"9ooZhREHlV77"}},{"cell_type":"code","source":["def dfColumnsToJoin(gdf):\n","  \"\"\"\n","  Converte para a coordenada desejada e adiciona as colunas utilizadas para \n","  realizar o join por Distancia.\n","  \"\"\"\n","  # Converte para WGS 84\n","  gdf.to_crs(crs = 'EPSG: 4326', inplace = True)\n","  # cria uma tupla com as coordenadas\n","  gdf['col_lat_long'] = list(zip(gdf.geometry.values.y, gdf.geometry.values.x))\n","  gdf['dist_ccdc'] = '' # receberá as distancias para as células\n","  gdf['coord_ccdc'] = ''\n","  return gdf"],"metadata":{"id":"fU4eu1UmldAF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Verificar as distancias"],"metadata":{"id":"vZjKx6H8cblw"}},{"cell_type":"code","source":["def ckdnearest(gdA, gdB):\n","\n","    nA = np.array(list(gdA.geometry.apply(lambda x: (x.x, x.y))))\n","    nB = np.array(list(gdB.geometry.apply(lambda x: (x.x, x.y))))\n","    btree = cKDTree(nB)\n","    dist, idx = btree.query(nA, k=1)\n","    gdB_nearest = gdB.iloc[idx].drop(columns=\"geometry\").reset_index(drop=True)\n","    gdf = pd.concat(\n","        [\n","            gdA.reset_index(drop=True),\n","            gdB_nearest,\n","            pd.Series(np.ceil(dist), name='distance')\n","        ], \n","        axis=1)\n","\n","    return gdf"],"metadata":{"id":"SjSxSJGfPMGN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  verifica a distancia entre os pontos\n","def distance_from(loc1,loc2):\n","  \"\"\"\n","  Entrada:\n","  loc1 - tupla com Coordenada do ponto do analista (lat, long)\n","  loc2 - tupla com Coordenada da celula do CCDC (lat, long)\n","  Saida:\n","  dist - A distancia entre os pontos\n","  loc2 - Isso precisa ser inserido no Data Frame de validacao, caso contrario\n","  não há uma coluna igual nos dois Data Frames que possibilite fazer o join\n","  \"\"\" \n","  dist = hs.haversine(loc1,loc2, unit=Unit.METERS)\n","  return (round(dist,8),loc2)"],"metadata":{"id":"2rs5CCrYcgKJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Adiciona as distancias da celula mais proxima aos pontos de validacao e todas as distancias do ponto nas celulas do df ccdc"],"metadata":{"id":"WZRNApYm77vP"}},{"cell_type":"code","source":["def addDist(points, dfccdc, i): \n","  \"\"\"\n","  Verifica a distancia entre os pontos dos data frames dados de entrada\n","  Entrada:\n","   - points: data frame com os pontos de analise\n","   - dfccdc: data frame com o resultado do ccdc\n","   - i: inteiro para indicar a linha do df 'points' que se quer a distancia\n","   Saida:\n","    - data frames de entrada atualizados com todas as distancias\n","  \"\"\"\n","  Plat, Plon = points.col_lat_long.loc[i]\n","  # cria uma lista temporaria contendo a distancia do ponto[i] para todas as\n","  #  coordenadas do dfccdc      \n","  tempList = dfccdc['coord_ccdc'].apply(lambda x: distance_from((Plat, Plon),x))\n","  # insere no df points a menor distancia encontrada e a coordenada do ponto\n","  #  mais proximo do dfccdc\n","  points['dist_ccdc'].loc[i] = min(tempList)[0]\n","  points['coord_ccdc'].loc[i] = min(tempList)[1]\n","  # insere no dfccdc a distancia de todas as coordenadas para o ponto[i] e \n","  # as coordenadas deste ponto\n","  dfccdc['Dist_Point'] = dfccdc['coord_ccdc'].apply(lambda x: distance_from((Plat, Plon),x)[0])\n","  dfccdc['Point_Val'] = points.geometry.loc[i]  \n","\n","  return points, dfccdc"],"metadata":{"id":"Q7z1SAUX25ck"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Gerar gráfico e obter o número de VPs para séries com finais variados - ANTIGO"],"metadata":{"id":"RGUQ8jslUp-N"}},{"cell_type":"code","source":["def countVPs(df, theta = 30, delta = (60, 90)):\n","  \"\"\"\n","  Entrada:\n","   - df: Data Frame filtrado\n","   - theta: inteiro,0 numero de  +/- dias permitidos entre o break e o analista,\n","         Padrão = 30\n","   - delta: tupla, intervalo de dias apos a data do analista para selecionar a\n","         truncagem do modelo, padrão = (60,90) dias\n","  Saida:\n","   - número de verdadeiros positivos que atendem as opções\n","  \"\"\"\n","  cond = np.array(((df.Dif_data >= datetime.timedelta(int(delta[0]))) & (df.Dif_data <= datetime.timedelta(int(delta[1])))))\n","  cond2 = np.array(((df.Dif_Z_Tbreak >= datetime.timedelta(-theta)) & (df.Dif_Z_Tbreak <= datetime.timedelta(theta))))\n","  df['Dif_data_Bool'] = cond\n","  df['Dif_Z_Tbreak_Bool'] = cond2\n","  VP = df[(df.Dif_data_Bool == True) & (df.Dif_Z_Tbreak_Bool == True )].value_counts('Dif_data_Bool').sum()\n","  return VP\n","\n","def graphVPs(df, theta = 30, delta = (60,90), graph = True):\n","  \"\"\"\n","  Para gerar o gráfico para vários finais diferentes de série é ncessário passar\n","  por um loop, então caso esta opção seja acessada os deltas serão definidos abaixo\n","  como um conjunto pré estabelecido.\n","  Caso se opte por não ter o gráfico só será retornado o número de VP que atende\n","  os parâmetros. \n","  Por padrão é gerado o gráfico\n","  \"\"\"\n","  df['Dif_data'] = df.End_S - df.data1_z\n","  df['Dif_Z_Tbreak'] = df.tBreak - df.data1_z\n","  #df.drop_duplicates(inplace = True)\n","  if graph:\n","    deltas = [[0,30],[30,60],[60,90],[90,120],[120,150],[150,180]]\n","    legenda = ['0 a 30 dias', '30 a 60 dias', '60 a 90 dias', '90 a 120 dias',\n","             '120 a 150 dias', '150 a 180 dias']\n","    qnts = []\n","    x=0\n","    for d in deltas:\n","      qnts.append([countVPs(df, theta, d), legenda[x]])\n","      x += 1\n","    grafDF = pd.DataFrame(qnts, columns=['VPs', 'Deltas'])\n","    fig, ax = plt.subplots(figsize=(13,5),dpi=80)\n","    matplotlib.style.use('ggplot')\n","    ax.scatter(grafDF.Deltas, grafDF.VPs, s = 10, c = 'y') \n","    plt.ylim(grafDF.VPs.min() - 5, grafDF.VPs.max() + 5)\n","    for i in range(grafDF.shape[0]):\n","      ax.annotate(grafDF.VPs.iloc[i], xy = (grafDF.Deltas.iloc[i], grafDF.VPs.iloc[i]))                \n","    ax.set_title('Número de VPs X Range de datas; Theta = {0}'.format(theta))\n","    ax.set_ylabel('Verdadeiros Positivos')\n","    ax.set_xlabel('Diferença entre o Fim da Série a data Z')\n","    ax.plot()\n","    return countVPs(df, theta, delta)  \n","  else:\n","    return countVPs(df, theta, delta) "],"metadata":{"id":"TpjPcYTsUz_e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Exportar TIF"],"metadata":{"id":"dFMWt1o_wuqn"}},{"cell_type":"code","source":["def exportTif(params_ImgCol, points_ee, params_ccdc, output_path, export_crs, export='countBreaks', year=False): \n","\n","  \"\"\"\n","  Creates and exports a raster with a subproduct of CCDC to the defined output path on google drive.\n","  Arguments:\n","    - params_ImgCol: parameters that define image collection;\n","    - points_ee: points to determine the location and extent of the study area;\n","    - params_ccdc: parameters used to run ccdc;\n","    - output_path: directory on google drive to which the raster will be saved;\n","    - export_crs: coordinate reference system of the output image;\n","    - export: defines what is the product to be exported. Options are: \n","              countBreaks - raster with total number of breaks per pixel;\n","              largestMagBreak - raster with date in milliseconds of break of largest magnitude, stored in band 1. Associated magnitude stored in band 2;\n","              latestBreak - raster with date in milliseconds of the most recent break (stored in band 1) and its associated magnitude (band 2);\n","              specificYearBreak - raster with date in milliseconds of break in a specific year, which should be entered as a parameter. Break stored in band 1 and magnitude in band 2;\n","    - year: year of analysis when export is dfined to specificYearBreak.\n","\n","  Function does not return anything, but saves raster to drive.\n","  \"\"\"\n","\n","  #create collection\n","  collection = getImageCollection(params_ImgCol, points_ee)\n","  #run ccdc\n","  ccdc_result = runCCDC(collection, params_ImgCol['bandas'], params_ccdc)\n","  #get tBreak and magnitude from ccdc_result\n","  tbreak = ccdc_result.select(['tBreak'])\n","  mag = ccdc_result.select(['{}_magnitude'.format(params_ImgCol['banda'])])\n","\n","   \n","  if export == 'countBreaks':\n","    result = tbreak.arrayLength(0).subtract(1).toInt16()\n","    msg = \"Generating raster with total number of breaks\"\n","  \n","  elif export == 'largestMagBreak':\n","    if type(year) is int and year in list(range(2000,2100)):    \n","      \"\"\"E.g ano agricola 2018 = out/2017 a set/2018, por isso year -1\"\"\" \n","      # mascara para o ano agricola desejado\n","      anoAgric = tbreak.gt(ee.Date(\"{}-10-01\".format(year - 1)).millis().getInfo()).And(tbreak.lt(ee.Date(\"{}-09-30\".format(year)).millis().getInfo()))\n","      negativo = mag.multiply(mag.lt(0.0)).multiply(anoAgric)\n","      # converter para valor absoluto\n","      magNeg = negativo.abs()\n","      # converte em array com os valores maximos\n","      argmaxNeg = magNeg.arrayArgmax()\n","      # converte para pixel normal\n","      argmaxNegBand = argmaxNeg.arrayFlatten([['argmax']])\n","      # pega os valores coorrespondentes a posicao do maior argumento\n","      arraygetNeg = negativo.arrayGet(argmaxNegBand)\n","      \n","      # data dos tBreaks associadas a mag negativa no ano agricola selecionado\n","      Tnegativo = tbreak.multiply(mag.lt(0.0)).multiply(anoAgric)\n","      dataNeg = Tnegativo.arrayGet(argmaxNegBand)\n","      \n","      # adicionar as duas informacoes em uma unica imagem com duas bandas\n","      result = arraygetNeg.addBands(dataNeg)\n","      msg = \"Generating raster with date in milliseconds of break of largest magnitude (band 1) and the associated magnitude (band 2)\"\n","    \n","    else:\n","      raise Exception(\"The argument year must be passed: an integer ranging from 2000 to 2099\")\n","    \n","  elif export == 'latestBreak':\n","\n","    #get argmax of the array band - this will capture the most recent break\n","    argmax = tbreak.arrayArgmax()\n","    #convert array pixel to normal pixel\n","    argmaxband = argmax.arrayFlatten([['argmax']])\n","    #get elements of the pixel array according to the argmax position\n","    get_tbreak = tbreak.arrayGet(argmaxband)\n","    get_mag = mag.arrayGet(argmaxband)\n","\n","    result = get_tbreak.addBands(get_mag)\n","\n","    msg = \"Generating raster with date in milliseconds of most recent break (band 1) and the associated magnitude of change (band 2)\"\n","\n","  elif export == 'specificYearBreak':\n","\n","    if type(year) is int and year in list(range(2000,2100)):\n","      \"\"\"E.g ano agricola 2018 = out/2017 a set/2018, porisso year -1\"\"\"\n","      #put 1 on array elements that belong to the agricultural year, otherwise put 0\n","      tbreak_year_flags = tbreak.gt(ee.Date(\"{}-10-01\".format(year-1)).millis().getInfo()).And(tbreak.lt(ee.Date(\"{}-09-30\".format(year)).millis().getInfo()))\n","      #get the position of 1s\n","      argmax = tbreak_year_flags.arrayArgmax()\n","      #convert array pixel to normal pixel\n","      argmaxband = argmax.arrayFlatten([['argmax']])\n","      #get elements from the original tbreak image at the obtained positions\n","      get_break_specific_year = tbreak.arrayGet(argmaxband)\n","      get_mag_specific_year = mag.arrayGet(argmaxband)\n","\n","      result = get_break_specific_year.addBands(get_mag_specific_year)\n","\n","      msg = \"Generating raster with date in milliseconds of breaks detected within the agricultural year of {} (band 1) and the associated magnitude of change (band 2)\".format(year)\n","    else:\n","      raise Exception(\"The argument year must be passed: an integer ranging from 2000 to 2099\")\n","\n","  \n","  else:\n","    raise Exception(\"Error: export parameter should be 'countBreaks', 'largestMagBreak', 'latestBreak' or 'specificYearBreak'.\")\n","\n","  #configure export parameters\n","  #export region\n","  export_region = points_ee.geometry().bounds()\n","\n","  #filename - the filename needs to be set firstly to a temp_name, then it will be renamed according to the naming conventions\n","  #if export == 'specificYearBreak':\n","  if export in ['specificYearBreak', 'largestMagBreak']:\n","    filename = fromParamsReturnNameTiff(params_ImgCol, params_ccdc, export_region, export)+'_{}'.format(year)\n","  else:\n","    filename = fromParamsReturnNameTiff(params_ImgCol, params_ccdc, export_region, export)\n","\n","  temp_name = 'temp_{}'.format(random.randint(0,9999))\n","\n","  #set folder to which GEE will export the image\n","  folder = os.path.split(output_path)[-1]\n","\n","  #export raster to drive\n","  task = ee.batch.Export.image.toDrive(**{\n","      'image':result,\n","      'description': temp_name, #filename,\n","      'folder':folder,\n","      'scale':10,\n","      'region': export_region,\n","      'crs': export_crs,\n","      'maxPixels':8030040147504\n","  })\n","\n","  task.start()\n","  t_start = time.time()\n","  print(msg)\n","  print('\\n')\n","  while task.active():\n","    print('\\rRunning export task... Time elapsed: {} min'.format(str(round((time.time()-t_start)/60,2))), end='')\n","    time.sleep(5)\n","\n","  #rename\n","  old_name = os.path.join(output_path, temp_name+'.tif')\n","  new_name = os.path.join(output_path, filename+'.tif')\n","  #check if file was already uploaded to google drive and then rename it\n","  found = False\n","  while not found:\n","    if os.path.exists(old_name):\n","      found = True\n","    time.sleep(1)\n","  os.rename(old_name, new_name)\n","  print('\\nExport finished')\n","\n"],"metadata":{"id":"Tn4vm62Sw6Un"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Funcoes auxiliares"],"metadata":{"id":"W6WXMyv0WAwW"}},{"cell_type":"markdown","source":["#####loadValidationPoints"],"metadata":{"id":"KsBCOZDcuB2Q"}},{"cell_type":"code","source":["def loadValidationPoints(points, crs='epsg:4326', id=False):\n","\n","  \"\"\"\n","  Returns the validation points as a geopandas dataframe and\n","  as an ee feature collection (or an ee feature in the case of 1 only point).\n","  \"\"\"\n","\n","  #execute if a path was entered\n","  if type(points) is str:\n","    if points.endswith('.shp') or points.endswith('.gpkg'):\n","      gdf_points = gpd.read_file(points)\n","    elif points.endswith('.csv'):\n","      gdf_points = gpd.GeoDataFrame(pd.read_csv(points), crs=crs)\n","    elif points.endswith('.pkl'):\n","      gdf_points = pd.read_pickle(points)\n","    #check if there is an id field in the gdf, then rename it to OBID. If there isn't, create an OBID field\n","    if 'OBID' not in gdf_points.columns:\n","      if 'id' in gdf_points.columns:\n","        gdf_points = gdf_points.rename(columns={\"id\": \"OBID\"})\n","      else:\n","        gdf_points['OBID'] = gdf_points.index\n","    #check if a specific id was entered\n","    if id is not False:\n","      gdf_points = gdf_points[gdf_points['OBID']==id]\n","    #convert gpd to feature collection\n","    ee_fc_points = geemap.geopandas_to_ee(gdf_points)\n","\n","  #execute if list of coordinates was entered\n","  elif type(points) is list:\n","\n","    if len(points)==1: #enter here if only one point was provided by the user\n","      #create gdf_points\n","      #create dataframe with Long, Lat and OBID\n","      temp_df = pd.DataFrame({'Long':[points[0][0]], 'Lat':[points[0][1]], 'OBID':0})\n","      #create geodataframe from dataframe \n","      gdf_points = gpd.GeoDataFrame(temp_df, geometry=gpd.points_from_xy(temp_df.Long, temp_df.Lat))\n","      #drop Long and Lat columns, then assign WGS84 CRS\n","      gdf_points.drop(columns=['Long','Lat'], inplace=True)\n","      gdf_points.crs = 'EPSG:4326'\n","\n","      #create ee_fc_points - note that in this case it is an ee.feature instead of a feature collection\n","      ee_fc_points = getSpecificPoint(points[0][0], points[0][1], 0)\n","    \n","    else: #in the case more than 1 points were entered by the user\n","      list_eeFeatures = []\n","      #adds ee Features (Points) to list\n","      for i in range(len(points)):\n","        list_eeFeatures.append(getSpecificPoint(points[i][0], points[i][1], i))\n","      #create feature collection\n","      ee_fc_points = ee.FeatureCollection(list_eeFeatures)\n","      #convert the feature collection to gdf\n","      gdf_points = geemap.ee_to_geopandas(ee_fc_points)\n","\n","  else:\n","    raise Exception(\"loadValidationPoints function argument should be a string with a path or a list of coordinates.\") \n","\n","  return gdf_points, ee_fc_points"],"metadata":{"id":"7bNsbpqJWC9h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#####getSpecificPoint"],"metadata":{"id":"gxJccnvct-yx"}},{"cell_type":"code","source":["def getSpecificPoint(long_ponto, lat_ponto, id):\n","\n","  \"\"\"\n","  Returns an ee.feature (Point) based on the coordinates entered. Also sets the id attribute.\n","  \"\"\"\n","\n","  ponto_especifico_ee = ee.Feature(ee.Geometry.Point([long_ponto,lat_ponto]), {'OBID':id})\n","\n","  return ponto_especifico_ee"],"metadata":{"id":"XiTce8Prcxh8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#####fromParamsReturnName"],"metadata":{"id":"v2dr5Cn3t67k"}},{"cell_type":"code","source":["def fromParamsReturnName(params_ImgCol, params_ccdc, Plon, Plat, buffer):\n","\n","  \"\"\"Returns the filename according to the parameters of execution.\"\"\"\n","\n","  #returns the name of the file (without extension) based on the parameters entered.\n","\n","  #get collection name\n","  col_name = \"S2SR\" if params_ImgCol['nameImage'] == \"COPERNICUS/S2_SR\" else params_ImgCol['nameImage']\n","  \n","  #get indices\n","  aux = ''\n","  params_ImgCol['indices'].sort()\n","  for i in params_ImgCol['indices']:\n","    indices = aux + i\n","    aux = indices + '-'\n","  indices = indices.upper()\n","\n","  #get ccdc params\n","  #get tmask bands\n","  tmask = ''\n","  for i in params_ccdc['bandas_tmask']:\n","    tmask = tmask + i\n","  tmask = tmask.upper()\n","  #get chi\n","  chi = str(params_ccdc['chiSquare'])\n","  chi = chi[chi.find('.')+1:]\n","  #get minYears\n","  minYears = str(params_ccdc['minYears']).replace('.','')\n","  #get num obs\n","  n_obs = str(params_ccdc['minObs'])\n","  #get lambda\n","  lam = str(params_ccdc['Lambda'])\n","  #get max iter\n","  maxIter = str(params_ccdc['maxIter'])\n","\n","  #get point lat/lon -> format: IDDDDD, IIDDDDD or IIIDDDDD followed by N, S, E or W (N, E for positive, S, W for negative)\n","  lat = \"{:7.5f}N\".format(Plat).replace('.','') if Plat > 0 else \"{:7.5f}S\".format(abs(Plat)).replace('.','')\n","  lon = \"{:7.5f}E\".format(Plon).replace('.','') if Plon > 0 else \"{:7.5f}W\".format(abs(Plon)).replace('.','')\n","\n","  #get buffer size\n","  buf = 0 if buffer is False else str(buffer) \n","\n","  #get dates\n","  date_start = params_ImgCol['date_start'].replace('-','')\n","  date_end = params_ImgCol['date_end'].replace('-','')\n","\n","  #get cloudmask\n","  cmask = 'S2CLESS' if params_ImgCol['cloudFilter'] == 's2cloudless' else params_ImgCol['cloudFilter'].upper()\n","  \n","\n","  name = \"{0}-{1}_{2}_TMASK{3}XX{4}YM{5}NOBS{6}LDA{7}ITER{8}_LON{9}_LAT{10}_BUF{11}_START{12}_END{13}\".format(col_name, cmask, indices, tmask, chi, minYears, n_obs, lam, maxIter,\n","                                                                                                        lon, lat, buf, date_start, date_end)\n","  \n","  return name"],"metadata":{"id":"JrKprO8mIXEj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#####fromParamsReturnNameTiff"],"metadata":{"id":"3Z37BYXqt26h"}},{"cell_type":"code","source":["def fromParamsReturnNameTiff(params_ImgCol, params_ccdc, export_region, var):\n","\n","  \"\"\"Returns the filename according to the parameters of execution.\n","  export_region must be a geometry\"\"\"\n","\n","  #returns the name of the file (without extension) based on the parameters entered.\n","\n","  #get collection name\n","  col_name = \"S2SR\" if params_ImgCol['nameImage'] == \"COPERNICUS/S2_SR\" else params_ImgCol['nameImage']\n","  \n","  #get indices\n","  aux = ''\n","  params_ImgCol['indices'].sort()\n","  for i in params_ImgCol['indices']:\n","    indices = aux + i\n","    aux = indices + '-'\n","  indices = indices.upper()\n","\n","  #get ccdc params\n","  #get tmask bands\n","  tmask = ''\n","  for i in params_ccdc['bandas_tmask']:\n","    tmask = tmask + i\n","  tmask = tmask.upper()\n","  #get chi\n","  chi = str(params_ccdc['chiSquare'])\n","  chi = chi[chi.find('.')+1:]\n","  #get minYears\n","  minYears = str(params_ccdc['minYears']).replace('.','')\n","  #get num obs\n","  n_obs = str(params_ccdc['minObs'])\n","  #get lambda\n","  lam = str(params_ccdc['Lambda'])\n","  #get max iter\n","  maxIter = str(params_ccdc['maxIter'])\n","\n","  #get point lat/lon -> format: IDDDDD, IIDDDDD or IIIDDDDD followed by N,S,E or W (N,E for positive, S,W for negative)\n","  bounds = export_region #export_region.geometry().bounds()\n","  listCoords = ee.Array.cat(bounds.coordinates(), 1)\n","  xCoords = listCoords.slice(1, 0, 1)\n","  yCoords = listCoords.slice(1, 1, 2)\n","  lonmin = xCoords.reduce('min', [0]).get([0,0]).getInfo()\n","  lonmax = xCoords.reduce('max', [0]).get([0,0]).getInfo()\n","  latmin = yCoords.reduce('min', [0]).get([0,0]).getInfo()\n","  latmax = yCoords.reduce('max', [0]).get([0,0]).getInfo()\n","\n","  lonmin = \"{:7.5f}E\".format(lonmin).replace('.','') if lonmin > 0 else \"{:7.5f}W\".format(abs(lonmin)).replace('.','')\n","  lonmax = \"{:7.5f}E\".format(lonmax).replace('.','') if lonmax > 0 else \"{:7.5f}W\".format(abs(lonmax)).replace('.','')\n","  latmin = \"{:7.5f}N\".format(latmin).replace('.','') if latmin > 0 else \"{:7.5f}S\".format(abs(latmin)).replace('.','')\n","  latmax = \"{:7.5f}N\".format(latmax).replace('.','') if latmax > 0 else \"{:7.5f}S\".format(abs(latmax)).replace('.','')\n","\n","  #get dates\n","  date_start = params_ImgCol['date_start'].replace('-','')\n","  date_end = params_ImgCol['date_end'].replace('-','')\n","\n","\n","\n","  name = \"{0}_{1}_{2}_TMASK{3}XX{4}YM{5}NOBS{6}LDA{7}ITER{8}_LONMIN{9}_LONMAX{10}_LATMIN{11}_LATMAX{12}_START{13}_END{14}_VAR{15}\".format(\n","                                                                                      col_name, params_ImgCol['banda'], indices, tmask, chi, minYears, n_obs, lam, maxIter,\n","                                                                                                        lonmin, lonmax, latmin, latmax, date_start, date_end, var)\n","  \n","  return name"],"metadata":{"id":"kKPMXMhps5qz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#####deleteItems"],"metadata":{"id":"nTSOl_dRtxtx"}},{"cell_type":"code","source":["def deleteItems(path, arg):\n","  \"\"\"\n","  Deletar varios itens do drive\n","  - entrada:\n","  path: pasta onde os arquivos se encontram\n","  arg: argumentos que devem constar no nome,\n","       e.g '*210*.csv', encontra todos os arquivos que tenham 210 no nome e terminem com .csv\n","  - saida: não há\n","  \"\"\"\n","  os.chdir(path) # transforma a pasta de entrada no atual workspace\n","  listaArquivo = glob.glob(arg) # procura na pasta de workspace todos os arquivos que contenham o argumento dado\n","  for arquivo in listaArquivo:\n","    print('Removido:', arquivo)\n","    os.remove(arquivo)"],"metadata":{"id":"-6ccOTizI_YN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Compositos"],"metadata":{"id":"fMDGSjIUqxM-"}},{"cell_type":"markdown","source":["####createComposite"],"metadata":{"id":"XsyVNv4zq4QI"}},{"cell_type":"code","source":["def fillImgColGapsWithCCDC(params_ImgCol, bands, params_ccdc, points_ee, buf=False):\n","\n","  \"\"\"\n","  Function that fills gaps in an image collection based on CCDC fitted segments. \n","  Returns a gap free image collection.\n","  Arguments:\n","  params_ImgCol - parameters necessary to initilize image collection;\n","  bands - bands that should be added to the composite;\n","  params_ccdc - parameters used to run CCDC;\n","  points_ee - ee feature containing the points of interest;\n","  buf - whether to process the information for a buffer around the points.\n","  \"\"\"\n","  #transform bands into an ee.List object\n","  bandList = ee.List(bands)\n","\n","  #initialize image collection\n","  imgCollection = getImageCollection(params_ImgCol, points_ee, buf=buf)\n","\n","  #select bands of interest\n","  imgCollection = imgCollection.select(bands)\n","\n","  #run ccdc\n","  ccdc = runCCDC(imgCollection, bands, params_ccdc)\n","\n","  #add time band to images\n","  def addTime(img):\n","    time = ee.Image(img.date().millis()).rename('t').float();\n","    return img.addBands(time)\n","  \n","  imgCollection =  imgCollection.map(addTime)\n","\n","  harmonicFrequencies = ee.List.sequence(1, 3);\n","\n","  def getCoefs(img):\n","    #get position that represents the segment\n","    segment = img.select('t').gt(ccdc.select('tEnd')).Not().arrayArgmax().arrayFlatten([['tEnd']])\n","    t_end_check = img.select('t').gt(ccdc.select('tEnd').arrayGet(-1))\n","    segment = segment.where(t_end_check, ccdc.select('tEnd').arrayLength(0).add(-1))\n","\n","    #make an image of frequencies\n","    frequencies = ee.Image.constant(harmonicFrequencies);\n","    #calculate time in randians (considering time is originally in milliseconds)\n","    PI2 = 2.0 * np.pi\n","    time = ee.Image(img).select('t').multiply( PI2 / (1000 * 60 * 60 * 24 * 365.25))\n","    #get the cosine terms\n","    cosines = time.multiply(frequencies).cos().rename(['cos1','cos2','cos3'])\n","    #get the sin terms\n","    sines = time.multiply(frequencies).sin().rename(['sin1','sin2','sin3'])\n","    #compute terms array\n","    terms = ee.Image.cat(ee.Image.constant(1),img.select('t'),cosines.select('cos1'),sines.select('sin1'),\n","                                                          cosines.select('cos2'),sines.select('sin2'),\n","                                                          cosines.select('cos3'),sines.select('sin3')).toArray().rename('terms')\n","    #get coefficients per band, multiply by their correspondent terms (sin, cos, slope, interception) and compute sum\n","    def getCoefsPerBand(b):\n","      return ccdc.select(ee.String(b).cat('_coefs')).arraySlice(0,segment,segment.add(1)).arrayReshape(ee.Image.constant(ee.Array([8])),1).multiply(terms).arrayReduce(ee.Reducer.sum(),[0]).arrayGet(0).rename(ee.String(b).cat('_fitted'))                                                    \n","    bands_coef = bandList.map(getCoefsPerBand)\n","      \n","    def addBandsIter(b,previous):\n","      return ee.Image(previous).addBands(b)\n","    merged = bands_coef.flatten().iterate(addBandsIter, ee.Image())\n","    \n","    return img.addBands(merged)\n","\n","  fittedCol = imgCollection.map(getCoefs)\n","  \n","  def fillGaps(img):\n","    def addBandsIter2(b,previous):\n","      return ee.Image(previous).addBands(img.select(ee.String(b)).unmask(img.select(ee.String(b).cat('_fitted'))))\n","    temp = img.select('t').addBands(bandList.iterate(addBandsIter2, ee.Image()))\n","                  \n","    return temp.select(bandList)\n","\n","  #mask\n","  mask_ccdc = ccdc.select('tStart').mask()\n","  def maskMasked(img):\n","    return img.updateMask(mask_ccdc)\n","  fittedCol = fittedCol.map(maskMasked)\n","\n","  return fittedCol.map(fillGaps)\n"],"metadata":{"id":"AyOjVoLXG1Tt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def createComposite(imgCollection, composite_period):\n","\n","  \"\"\"\n","  Function that creates monthly composites (median) of an image collection, returns an ee.Image.\n","  Arguments: \n","  imgCollection - image collection (time series) used to create composites;\n","  composite_period - a dictionary that contains the dates of start and end, determining the time period\n","                  that the composite will cover.\n","  \"\"\"\n","\n","  #get start and end years\n","  year_start = ee.Date(composite_period['date_start']).get('year')\n","  year_end = ee.Date(composite_period['date_end']).get('year')\n","\n","  #create list with 12 months and the intended years\n","  months = ee.List.sequence(1,12)\n","  years = ee.List.sequence(year_start,year_end)\n","\n","  ############### auxiliary functions ###############\n","  #block 1 - create composites and rename images of the composite\n","  #auxiliary functions to run the createComposite function (they have to be defined within the function to allow the use of the map operator)\n","  def makeCompositesPerYear(y):\n","\n","    def makeMonthlyComposites(m):\n","      return imgCollection.filter(ee.Filter.calendarRange(y,y,'year')).filter(ee.Filter.calendarRange(m,m,'month')).median().int().set('month',m,'year',y)\n","\n","    return ee.ImageCollection.fromImages(months.map(makeMonthlyComposites))\n","  \n","  def decomposeList(l):\n","    return ee.ImageCollection(l).toList(12)\n","\n","  def renameImages(img):\n","    eeimg = ee.Image(img)\n","    value = ee.Number(eeimg.get('year')).format('%04d').cat('_').cat(ee.Number(eeimg.get('month')).format('%02d'))\n","    return ee.Image(eeimg.set('system:index',value,'system:id',value))\n","\n","  #block 2 - set and add image time\n","  #auxiliary function to set the time of the image, set number of bands and add a time band\n","  def setTimeAndNumBands(img):\n","    return img.set('system:time_start', ee.Date.fromYMD(img.get('year'),img.get('month'),1).millis(), 'num_bands', img.bandNames().length())\n","  \n","  ############### end of auxiliary functions ###############\n","\n","  #execution of block 1 - creation of the composite\n","  #make composites per year\n","  list_years = years.map(makeCompositesPerYear)\n","\n","  #decompose resulting img collection to list of images\n","  list_imgs = list_years.map(decomposeList).flatten()\n","\n","  #rename images (format name as YYYY_MM)\n","  list_imgs_renamed = list_imgs.map(renameImages)\n","\n","  #convert from list to img collection\n","  decomposed_collection = ee.ImageCollection(list_imgs_renamed)\n","\n","  #transform images of the image collection in bands of single image \n","  #composite_in_bands = decomposed_collection.toBands()\n","\n","  #execution of block 2 - add variables\n","  #decomposed_collection = decomposed_collection.map(addVariables)\n","  \n","  decomposed_collection = decomposed_collection.map(setTimeAndNumBands)\n","  decomposed_collection = decomposed_collection.filter('num_bands > 0')\n","\n","  return decomposed_collection.toBands()#.select(band_names)\n","\n"],"metadata":{"id":"Z4iTRC9QqyC-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Extrair info dos compósitos"],"metadata":{"id":"RFXJfrsIq99I"}},{"cell_type":"code","source":["#OBS - FUNCAO NÃO APROPRIADA PARA GERAR O DATAFRAME TOTAL, POIS NECESSITA DE MUITO TEMPO DE EXECUÇÃO\n","#ESTÁ AQUI APENAS COMO MATERIAL DE BASE PARA FUTUROS AJUSTES/ADAPTAÇÕES, SE NECESSÁRIO\n","#O PROCEDIMENTO UTILIZADO PARA GERAR OS COMPÓSITOS SALVOS NO DRIVE É DESCRITO NO MAIN\n","def extractCompositeInfo(df, params_ImgCol, composite_period, bands_composite, params_ccdc, points_ee, buf=False):\n","\n","  #compute gap free image collection\n","  filledCol = fillImgColGapsWithCCDC(params_ImgCol, bands_composite, params_ccdc, points_ee, buf=False)\n","\n","  #generate composite\n","  composite = createComposite(filledCol, composite_period)\n","\n","  #initialize empty dataframe\n","  df_composites = pd.DataFrame()\n","\n","  #adjust coordinate column of the dataframe - if it is str, convert to tuple\n","  #if type(df.coord_ccdc.head(1)[0]) is str:\n","    #df.coord_ccdc = df.coord_ccdc.apply(eval)\n","  \n","  #transform to WGS84 if in other crs\n","  if df.crs.name != 'WGS 84':\n","    df = df.to_crs('epsg:4326')\n","\n","  #loop through dataframe rows and get lat lon\n","  #for Plat, Plon in list(df.coord_ccdc):\n","  for Plat, Plon in list(zip(df.geometry.y.values, df.geometry.x.values)):\n","    print(Plon, Plat)\n","    \n","    if buf:\n","      target_geometry = ee.Feature(ee.Geometry.Point([Plon, Plat])).buffer(buf).geometry()\n","    else:\n","      target_geometry = ee.Geometry.Point([Plon, Plat]) \n","\n","    #unmask - fill masked elements with an arbitrary value\n","    composite = composite.unmask(32767)\n","\n","    #apply reduceRegion\n","    reduced = composite.reduceRegion(\n","        reducer= ee.Reducer.toList(),    \n","        geometry = target_geometry,#ee.Geometry.Point([Plon,Plat]),    \n","        scale= 10)  \n","    \n","    #store in temporary dataframe\n","    df_temp = pd.DataFrame(reduced.getInfo()) \n","    print('chegou aqui')\n","    #add coordinates\n","    dicLatLong = ee.Image.pixelLonLat().updateMask(composite.mask().reduce('anyNonZero')).reduceRegion(\n","      reducer= ee.Reducer.toList(),\n","      geometry = target_geometry,\n","      scale= 10)\n","    #convert to data frame\n","    df_Lat_Long = pd.DataFrame(dicLatLong.getInfo())\n","    #join\n","    df_temp = df_temp.join(df_Lat_Long)\n","\n","    #append temporary dataframe to initial dataframe\n","    df_composites = df_composites.append(df_temp)\n","    print('completed')\n"," \n","    \n","  return df_composites"],"metadata":{"id":"uK4OwFuyrBEz"},"execution_count":null,"outputs":[]}]}